"""
å®Œæ•´çš„6é˜¶æ®µé«˜çº§è®­ç»ƒç³»ç»Ÿ
é›†æˆæ‰€æœ‰ä¼˜åŒ–ç»„ä»¶ï¼šé«˜çº§ç»ˆç‚¹å¼•å¯¼ã€ä¼˜åŒ–è¯¾ç¨‹å­¦ä¹ ã€æ™ºèƒ½å¥–åŠ±ç³»ç»Ÿ
ç¡®ä¿èƒ½å¤Ÿå®Œæˆæ‰€æœ‰6ä¸ªé˜¶æ®µ
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Any, Tuple
import time
from datetime import datetime

# è®¾ç½®matplotlibä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# ç¡®ä¿srcæ¨¡å—è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.join(current_dir, 'src')
sys.path.insert(0, src_path)

from stable_baselines3 import DQN
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback

from environment.uav_env import UAVEnvironment
from environment.advanced_endpoint_guidance import AdvancedEndpointGuidanceConfig, AdvancedEndpointGuidanceCalculator
from environment.optimized_6stage_curriculum import Optimized6StageConfig, Optimized6StageManager, Optimized6StageRewardCalculator
from environment.intelligent_reward_system import IntelligentRewardConfig, IntelligentRewardSystem


class Advanced6StageCallback(BaseCallback):
    """
    é«˜çº§6é˜¶æ®µè®­ç»ƒå›è°ƒ
    """
    
    def __init__(self, stage_manager: Optimized6StageManager, 
                 intelligent_system: IntelligentRewardSystem, verbose: int = 1):
        super().__init__(verbose)
        self.stage_manager = stage_manager
        self.intelligent_system = intelligent_system
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_count = 0
        self.stage_history = []
        self.success_history = []
        self.reward_history = []
        
        # æ€§èƒ½è¿½è¸ª
        self.training_start_time = None
        self.stage_start_times = {}
        self.best_performance = {}
        
    def _on_training_start(self) -> None:
        """è®­ç»ƒå¼€å§‹æ—¶è°ƒç”¨"""
        self.training_start_time = time.time()
        self.stage_start_times[self.stage_manager.current_stage] = time.time()
        
        print("ğŸš€ === å¯åŠ¨å®Œæ•´6é˜¶æ®µé«˜çº§è®­ç»ƒç³»ç»Ÿ === ğŸš€")
        print(f"é›†æˆç»„ä»¶: é«˜çº§ç»ˆç‚¹å¼•å¯¼ + ä¼˜åŒ–è¯¾ç¨‹å­¦ä¹  + æ™ºèƒ½å¥–åŠ±ç³»ç»Ÿ")
        print(f"ç›®æ ‡: å®Œæˆæ‰€æœ‰6ä¸ªé˜¶æ®µçš„è¯¾ç¨‹å­¦ä¹ ")
        
        current_stage_info = self.stage_manager.get_current_stage_info()
        print(f"\nğŸ“ å¼€å§‹é˜¶æ®µ: {current_stage_info['stage_name']}")
        print(f"   ç›®æ ‡: {current_stage_info.get('description', 'N/A')}")
        print(f"   ç”¨æˆ·ä½ç½®: {current_stage_info['user_positions'][:, :2].tolist()}")
        print(f"   æœŸæœ›æˆåŠŸç‡: {current_stage_info.get('expected_success_rate', 0):.1%}")
    
    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨"""
        return True
    
    def _on_rollout_end(self) -> None:
        """æ¯ä¸ªrolloutç»“æŸæ—¶è°ƒç”¨"""
        # è·å–æœ€æ–°å›åˆä¿¡æ¯
        if hasattr(self.training_env, 'get_episode_rewards'):
            episode_rewards = self.training_env.get_episode_rewards()
            if len(episode_rewards) > len(self.reward_history):
                # æ–°å›åˆå®Œæˆ
                self.episode_count += 1
                latest_reward = episode_rewards[-1]
                self.reward_history.append(latest_reward)
                
                # è·å–å›åˆç»“æœï¼ˆä»environment infoä¸­ï¼‰
                if hasattr(self.training_env.unwrapped, '_get_info'):
                    info = self.training_env.unwrapped._get_info()
                    self._process_episode_result(info, latest_reward)
    
    def _process_episode_result(self, info: Dict[str, Any], reward: float):
        """å¤„ç†å›åˆç»“æœ"""
        # æ„å»ºå›åˆç»“æœ
        episode_result = {
            'total_reward': reward,
            'users_visited': info.get('users_visited', 0),
            'reached_end': info.get('reached_end', False),
            'current_time': info.get('current_time', 0),
            'uav_position': info.get('uav_position', [0, 0, 0])
        }
        
        # æ›´æ–°æ™ºèƒ½ç³»ç»Ÿæ€§èƒ½
        self.intelligent_system.update_episode_performance(episode_result)
        
        # æ£€æŸ¥ä»»åŠ¡æˆåŠŸ
        success = (episode_result['users_visited'] >= 1 and episode_result['reached_end'])
        self.success_history.append(success)
        
        # è®°å½•é˜¶æ®µä¿¡æ¯
        current_stage = self.stage_manager.current_stage
        self.stage_history.append(current_stage)
        
        # è¯„ä¼°æ˜¯å¦éœ€è¦è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
        should_advance = self.stage_manager.evaluate_stage_performance(episode_result)
        
        if should_advance:
            self._handle_stage_advancement()
        
        # å®æ—¶åé¦ˆï¼ˆæ¯10å›åˆï¼‰
        if self.episode_count % 10 == 0:
            self._print_progress_update()
    
    def _handle_stage_advancement(self):
        """å¤„ç†é˜¶æ®µæ™‹çº§"""
        completed_stage = self.stage_manager.current_stage
        stage_time = time.time() - self.stage_start_times.get(completed_stage, time.time())
        
        # è®°å½•é˜¶æ®µæ€§èƒ½
        stage_info = self.stage_manager.get_current_stage_info()
        stage_performance = {
            'stage': completed_stage,
            'episodes': stage_info['episodes'],
            'success_rate': stage_info['success_rate'],
            'training_time': stage_time,
            'avg_reward': stage_info['avg_reward']
        }
        self.best_performance[f'stage_{completed_stage}'] = stage_performance
        
        # æ›´æ–°ç¯å¢ƒç”¨æˆ·ä½ç½®
        if not self.stage_manager.is_curriculum_complete():
            self.stage_manager.advance_to_next_stage()
            new_stage_config = self.stage_manager.get_stage_config(self.stage_manager.current_stage)
            
            # æ›´æ–°ç¯å¢ƒ
            if hasattr(self.training_env.unwrapped, 'user_manager'):
                self.training_env.unwrapped.user_manager.set_user_positions(
                    new_stage_config['user_positions']
                )
            
            # è®°å½•æ–°é˜¶æ®µå¼€å§‹æ—¶é—´
            self.stage_start_times[self.stage_manager.current_stage] = time.time()
        else:
            # è¯¾ç¨‹å­¦ä¹ å®Œæˆ
            self._handle_curriculum_completion()
    
    def _handle_curriculum_completion(self):
        """å¤„ç†è¯¾ç¨‹å­¦ä¹ å®Œæˆ"""
        total_time = time.time() - self.training_start_time
        
        print(f"\nğŸ† === 6é˜¶æ®µè¯¾ç¨‹å­¦ä¹ å®Œæˆï¼=== ğŸ†")
        print(f"æ€»è®­ç»ƒæ—¶é—´: {total_time/60:.1f}åˆ†é’Ÿ")
        print(f"æ€»è®­ç»ƒå›åˆ: {self.episode_count}")
        print(f"æ€»ä½“æˆåŠŸç‡: {np.mean(self.success_history[-50:]):.2%}")
        
        # ç”Ÿæˆå®ŒæˆæŠ¥å‘Š
        self._generate_completion_report()
    
    def _print_progress_update(self):
        """æ‰“å°è¿›åº¦æ›´æ–°"""
        current_stage_info = self.stage_manager.get_current_stage_info()
        recent_success_rate = np.mean(self.success_history[-10:]) if len(self.success_history) >= 10 else 0
        recent_reward = np.mean(self.reward_history[-10:]) if len(self.reward_history) >= 10 else 0
        
        print(f"\nğŸ“Š === ç¬¬{self.episode_count}å›åˆè¿›åº¦æŠ¥å‘Š === ğŸ“Š")
        print(f"å½“å‰é˜¶æ®µ: {current_stage_info['stage_name']} ({current_stage_info['stage']}/6)")
        print(f"é˜¶æ®µè¿›åº¦: {current_stage_info['episodes']}å›åˆ | æˆåŠŸç‡: {current_stage_info['success_rate']:.2%}")
        print(f"æœ€è¿‘10å›åˆ: æˆåŠŸç‡ {recent_success_rate:.2%} | å¹³å‡å¥–åŠ± {recent_reward:.0f}")
        
        # æ™ºèƒ½ç³»ç»ŸçŠ¶æ€
        system_stats = self.intelligent_system.get_system_stats()
        performance_stats = system_stats['performance_stats']
        print(f"æ™ºèƒ½ç³»ç»Ÿ: æˆåŠŸç‡ {performance_stats['success_rate']:.2%} | "
              f"è¶‹åŠ¿ {performance_stats['performance_trend']:+.2f} | "
              f"å¹²é¢„æ¬¡æ•° {system_stats['recent_interventions']}")
    
    def _generate_completion_report(self):
        """ç”Ÿæˆå®ŒæˆæŠ¥å‘Š"""
        report_file = f"6stage_completion_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("=== 6é˜¶æ®µè¯¾ç¨‹å­¦ä¹ å®ŒæˆæŠ¥å‘Š ===\n\n")
            f.write(f"è®­ç»ƒå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»è®­ç»ƒå›åˆ: {self.episode_count}\n")
            f.write(f"æ€»ä½“æˆåŠŸç‡: {np.mean(self.success_history):.2%}\n\n")
            
            f.write("å„é˜¶æ®µæ€§èƒ½:\n")
            for stage_name, performance in self.best_performance.items():
                f.write(f"  {stage_name}: {performance['episodes']}å›åˆ, "
                       f"æˆåŠŸç‡{performance['success_rate']:.2%}, "
                       f"è®­ç»ƒæ—¶é—´{performance['training_time']/60:.1f}åˆ†é’Ÿ\n")
            
            f.write(f"\næŠ¥å‘Šä¿å­˜ä½ç½®: {os.path.abspath(report_file)}")
        
        print(f"ğŸ“„ å®ŒæˆæŠ¥å‘Šå·²ä¿å­˜: {report_file}")


def create_advanced_6stage_environment() -> Tuple[UAVEnvironment, Optimized6StageManager, IntelligentRewardSystem]:
    """
    åˆ›å»ºé«˜çº§6é˜¶æ®µç¯å¢ƒ
    """
    print("ğŸ› ï¸ åˆ›å»ºé«˜çº§6é˜¶æ®µç¯å¢ƒ...")
    
    # 1. åˆ›å»ºé…ç½®
    stage_config = Optimized6StageConfig()
    intelligent_config = IntelligentRewardConfig()
    
    # 2. åˆ›å»ºé˜¶æ®µç®¡ç†å™¨
    stage_manager = Optimized6StageManager(stage_config)
    
    # 3. åˆ›å»ºæ™ºèƒ½å¥–åŠ±ç³»ç»Ÿ
    intelligent_system = IntelligentRewardSystem(intelligent_config, stage_config, stage_manager)
    
    # 4. åˆ›å»ºç¯å¢ƒ
    env = UAVEnvironment(
        env_size=(100, 100, 50),
        num_users=2,
        num_antennas=8,
        start_position=(0, 0, 50),
        end_position=(80, 80, 50),
        flight_time=250.0,
        time_step=0.1,
        transmit_power=0.5,
        max_speed=30.0,
        min_speed=10.0,
        fixed_users=True,
        seed=42
    )
    
    # 5. è®¾ç½®ç¬¬ä¸€é˜¶æ®µçš„ç”¨æˆ·ä½ç½®
    stage1_config = stage_manager.get_stage_config(1)
    env.user_manager.set_user_positions(stage1_config['user_positions'])
    
    # 6. é›†æˆæ™ºèƒ½å¥–åŠ±ç³»ç»Ÿåˆ°ç¯å¢ƒ
    class IntelligentRewardCalculator:
        def __init__(self, intelligent_system):
            self.intelligent_system = intelligent_system
            
        def calculate_reward(self, **kwargs):
            return self.intelligent_system.calculate_intelligent_reward(**kwargs)
        
        def reset(self):
            self.intelligent_system.reset()
        
        def get_stats(self):
            return self.intelligent_system.get_system_stats()
    
    env.set_reward_calculator(IntelligentRewardCalculator(intelligent_system))
    
    print("âœ… é«˜çº§6é˜¶æ®µç¯å¢ƒåˆ›å»ºå®Œæˆ")
    print(f"   - é˜¶æ®µç®¡ç†å™¨: {type(stage_manager).__name__}")
    print(f"   - æ™ºèƒ½å¥–åŠ±ç³»ç»Ÿ: {type(intelligent_system).__name__}")
    print(f"   - é›†æˆç»„ä»¶: ç»ˆç‚¹å¼•å¯¼ + è¯¾ç¨‹å­¦ä¹  + æ™ºèƒ½å¥–åŠ±")
    
    return env, stage_manager, intelligent_system


def create_enhanced_dqn_agent(env: UAVEnvironment) -> DQN:
    """
    åˆ›å»ºå¢å¼ºçš„DQNæ™ºèƒ½ä½“
    """
    print("ğŸ¤– åˆ›å»ºå¢å¼ºDQNæ™ºèƒ½ä½“...")
    
    agent = DQN(
        policy='MlpPolicy',
        env=env,
        learning_rate=5e-4,           # ä¼˜åŒ–å­¦ä¹ ç‡
        gamma=0.995,                  # é«˜æŠ˜æ‰£å› å­
        batch_size=128,               # å¤§æ‰¹æ¬¡è®­ç»ƒ
        buffer_size=400000,           # å¤§ç»éªŒç¼“å†²åŒº
        exploration_initial_eps=0.9,  # åˆå§‹æ¢ç´¢ç‡
        exploration_final_eps=0.05,   # æœ€ç»ˆæ¢ç´¢ç‡
        exploration_fraction=0.7,     # æ¢ç´¢è¡°å‡æ¯”ä¾‹
        learning_starts=2000,         # å¼€å§‹å­¦ä¹ çš„æ­¥æ•°
        train_freq=4,                 # è®­ç»ƒé¢‘ç‡
        target_update_interval=1000,  # ç›®æ ‡ç½‘ç»œæ›´æ–°é—´éš”
        policy_kwargs=dict(
            net_arch=[512, 256, 128, 64]  # æ·±åº¦ç½‘ç»œæ¶æ„
        ),
        verbose=1,
        seed=42
    )
    
    print("âœ… å¢å¼ºDQNæ™ºèƒ½ä½“åˆ›å»ºå®Œæˆ")
    print(f"   - ç½‘ç»œæ¶æ„: [512, 256, 128, 64]")
    print(f"   - å­¦ä¹ ç‡: {agent.learning_rate}")
    print(f"   - ç¼“å†²åŒºå¤§å°: {agent.buffer_size}")
    print(f"   - æ¢ç´¢ç­–ç•¥: {agent.exploration_initial_eps} â†’ {agent.exploration_final_eps}")
    
    return agent


def train_complete_6stage_system(total_timesteps: int = 200000):
    """
    è®­ç»ƒå®Œæ•´çš„6é˜¶æ®µç³»ç»Ÿ
    """
    print("ğŸ¯ === å¼€å§‹å®Œæ•´6é˜¶æ®µé«˜çº§è®­ç»ƒ === ğŸ¯")
    
    # 1. åˆ›å»ºç¯å¢ƒå’Œç³»ç»Ÿ
    env, stage_manager, intelligent_system = create_advanced_6stage_environment()
    monitored_env = Monitor(env)
    
    # 2. åˆ›å»ºæ™ºèƒ½ä½“
    agent = create_enhanced_dqn_agent(monitored_env)
    
    # 3. åˆ›å»ºé«˜çº§å›è°ƒ
    callback = Advanced6StageCallback(stage_manager, intelligent_system, verbose=1)
    
    # 4. å¼€å§‹è®­ç»ƒ
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒï¼Œç›®æ ‡æ­¥æ•°: {total_timesteps:,}")
    print(f"é¢„æœŸè®­ç»ƒæ—¶é—´: {total_timesteps/10000:.0f}-{total_timesteps/5000:.0f}åˆ†é’Ÿ")
    
    start_time = time.time()
    
    try:
        agent.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            progress_bar=True
        )
        
        training_time = time.time() - start_time
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time/60:.1f}åˆ†é’Ÿ")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        training_time = time.time() - start_time
        print(f"å·²è®­ç»ƒæ—¶é—´: {training_time/60:.1f}åˆ†é’Ÿ")
    
    # 5. ä¿å­˜æ¨¡å‹
    model_path = f"advanced_6stage_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    agent.save(model_path)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    # 6. ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
    generate_training_report(callback, stage_manager, intelligent_system, training_time)
    
    return agent, monitored_env, callback


def generate_training_report(callback: Advanced6StageCallback, 
                           stage_manager: Optimized6StageManager,
                           intelligent_system: IntelligentRewardSystem,
                           training_time: float):
    """
    ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
    """
    print(f"\nğŸ“Š === ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š === ğŸ“Š")
    
    # åˆ›å»ºç»“æœç›®å½•
    results_dir = f"results/6stage_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    os.makedirs(results_dir, exist_ok=True)
    
    # 1. è®­ç»ƒæ›²çº¿å›¾
    if len(callback.success_history) > 0:
        plot_training_curves(callback, results_dir)
    
    # 2. é˜¶æ®µæ€§èƒ½å›¾
    plot_stage_performance(callback, stage_manager, results_dir)
    
    # 3. æ™ºèƒ½ç³»ç»Ÿåˆ†æ
    plot_intelligent_system_analysis(intelligent_system, results_dir)
    
    # 4. ç»¼åˆæŠ¥å‘Š
    generate_comprehensive_report(callback, stage_manager, intelligent_system, 
                                training_time, results_dir)
    
    print(f"ğŸ“ è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {results_dir}")


def plot_training_curves(callback: Advanced6StageCallback, results_dir: str):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('6é˜¶æ®µè®­ç»ƒæ›²çº¿åˆ†æ', fontsize=16, fontweight='bold')
    
    # æˆåŠŸç‡æ›²çº¿
    if len(callback.success_history) > 0:
        window_size = min(20, len(callback.success_history) // 10)
        success_rate_smooth = []
        for i in range(window_size, len(callback.success_history)):
            success_rate_smooth.append(np.mean(callback.success_history[i-window_size:i]))
        
        axes[0, 0].plot(success_rate_smooth, 'b-', linewidth=2)
        axes[0, 0].set_title('æˆåŠŸç‡å˜åŒ–æ›²çº¿')
        axes[0, 0].set_xlabel('è®­ç»ƒå›åˆ')
        axes[0, 0].set_ylabel('æˆåŠŸç‡')
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_ylim(0, 1)
    
    # å¥–åŠ±æ›²çº¿
    if len(callback.reward_history) > 0:
        window_size = min(20, len(callback.reward_history) // 10)
        reward_smooth = []
        for i in range(window_size, len(callback.reward_history)):
            reward_smooth.append(np.mean(callback.reward_history[i-window_size:i]))
        
        axes[0, 1].plot(reward_smooth, 'g-', linewidth=2)
        axes[0, 1].set_title('å¥–åŠ±å˜åŒ–æ›²çº¿')
        axes[0, 1].set_xlabel('è®­ç»ƒå›åˆ')
        axes[0, 1].set_ylabel('å¹³å‡å¥–åŠ±')
        axes[0, 1].grid(True, alpha=0.3)
    
    # é˜¶æ®µåˆ†å¸ƒ
    if len(callback.stage_history) > 0:
        stage_counts = {}
        for stage in callback.stage_history:
            stage_counts[stage] = stage_counts.get(stage, 0) + 1
        
        stages = list(stage_counts.keys())
        counts = list(stage_counts.values())
        
        axes[1, 0].bar([f'é˜¶æ®µ{s}' for s in stages], counts, color='orange', alpha=0.7)
        axes[1, 0].set_title('å„é˜¶æ®µè®­ç»ƒåˆ†å¸ƒ')
        axes[1, 0].set_ylabel('è®­ç»ƒå›åˆæ•°')
        axes[1, 0].tick_params(axis='x', rotation=45)
    
    # é˜¶æ®µæˆåŠŸç‡å¯¹æ¯”
    if hasattr(callback, 'best_performance') and callback.best_performance:
        stages = []
        success_rates = []
        
        for stage_name, performance in callback.best_performance.items():
            stages.append(stage_name.replace('stage_', 'é˜¶æ®µ'))
            success_rates.append(performance['success_rate'])
        
        if stages:
            axes[1, 1].bar(stages, success_rates, color='purple', alpha=0.7)
            axes[1, 1].set_title('å„é˜¶æ®µæœ€ç»ˆæˆåŠŸç‡')
            axes[1, 1].set_ylabel('æˆåŠŸç‡')
            axes[1, 1].set_ylim(0, 1)
            axes[1, 1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, 'training_curves.png'), dpi=300, bbox_inches='tight')
    plt.close()


def plot_stage_performance(callback: Advanced6StageCallback, 
                         stage_manager: Optimized6StageManager, results_dir: str):
    """ç»˜åˆ¶é˜¶æ®µæ€§èƒ½å›¾"""
    if not hasattr(callback, 'best_performance') or not callback.best_performance:
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('6é˜¶æ®µæ€§èƒ½è¯¦ç»†åˆ†æ', fontsize=16, fontweight='bold')
    
    stages = []
    episodes = []
    success_rates = []
    avg_rewards = []
    training_times = []
    
    for stage_name, performance in callback.best_performance.items():
        stages.append(int(stage_name.split('_')[1]))
        episodes.append(performance['episodes'])
        success_rates.append(performance['success_rate'])
        avg_rewards.append(performance['avg_reward'])
        training_times.append(performance['training_time'] / 60)  # è½¬æ¢ä¸ºåˆ†é’Ÿ
    
    if stages:
        # å„é˜¶æ®µè®­ç»ƒå›åˆæ•°
        axes[0, 0].plot(stages, episodes, 'bo-', linewidth=2, markersize=8)
        axes[0, 0].set_title('å„é˜¶æ®µè®­ç»ƒå›åˆæ•°')
        axes[0, 0].set_xlabel('é˜¶æ®µ')
        axes[0, 0].set_ylabel('è®­ç»ƒå›åˆæ•°')
        axes[0, 0].grid(True, alpha=0.3)
        
        # å„é˜¶æ®µæˆåŠŸç‡
        axes[0, 1].plot(stages, success_rates, 'go-', linewidth=2, markersize=8)
        axes[0, 1].set_title('å„é˜¶æ®µæˆåŠŸç‡')
        axes[0, 1].set_xlabel('é˜¶æ®µ')
        axes[0, 1].set_ylabel('æˆåŠŸç‡')
        axes[0, 1].set_ylim(0, 1)
        axes[0, 1].grid(True, alpha=0.3)
        
        # å„é˜¶æ®µå¹³å‡å¥–åŠ±
        axes[1, 0].plot(stages, avg_rewards, 'ro-', linewidth=2, markersize=8)
        axes[1, 0].set_title('å„é˜¶æ®µå¹³å‡å¥–åŠ±')
        axes[1, 0].set_xlabel('é˜¶æ®µ')
        axes[1, 0].set_ylabel('å¹³å‡å¥–åŠ±')
        axes[1, 0].grid(True, alpha=0.3)
        
        # å„é˜¶æ®µè®­ç»ƒæ—¶é—´
        axes[1, 1].plot(stages, training_times, 'mo-', linewidth=2, markersize=8)
        axes[1, 1].set_title('å„é˜¶æ®µè®­ç»ƒæ—¶é—´')
        axes[1, 1].set_xlabel('é˜¶æ®µ')
        axes[1, 1].set_ylabel('è®­ç»ƒæ—¶é—´ (åˆ†é’Ÿ)')
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, 'stage_performance.png'), dpi=300, bbox_inches='tight')
    plt.close()


def plot_intelligent_system_analysis(intelligent_system: IntelligentRewardSystem, results_dir: str):
    """ç»˜åˆ¶æ™ºèƒ½ç³»ç»Ÿåˆ†æå›¾"""
    system_stats = intelligent_system.get_system_stats()
    performance_stats = system_stats['performance_stats']
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('æ™ºèƒ½å¥–åŠ±ç³»ç»Ÿåˆ†æ', fontsize=16, fontweight='bold')
    
    # æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾ï¼ˆç®€åŒ–ç‰ˆï¼‰
    metrics = ['æˆåŠŸç‡', 'å®Œæˆç‡', 'æ€§èƒ½è¶‹åŠ¿']
    values = [
        performance_stats.get('success_rate', 0),
        performance_stats.get('completion_rate', 0),
        max(0, performance_stats.get('performance_trend', 0))  # åªæ˜¾ç¤ºæ­£è¶‹åŠ¿
    ]
    
    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
    values += values[:1]  # é—­åˆå›¾å½¢
    angles += angles[:1]
    
    ax1.plot(angles, values, 'o-', linewidth=2, color='blue')
    ax1.fill(angles, values, alpha=0.25, color='blue')
    ax1.set_xticks(angles[:-1])
    ax1.set_xticklabels(metrics)
    ax1.set_title('ç³»ç»Ÿæ€§èƒ½é›·è¾¾å›¾')
    ax1.set_ylim(0, 1)
    
    # æƒé‡è°ƒæ•´å†å²ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    current_weights = system_stats.get('current_weights', {})
    if current_weights:
        weight_names = list(current_weights.keys())
        weight_values = list(current_weights.values())
        
        ax2.bar(range(len(weight_names)), weight_values, alpha=0.7, color='green')
        ax2.set_title('å½“å‰æƒé‡åˆ†å¸ƒ')
        ax2.set_xticks(range(len(weight_names)))
        ax2.set_xticklabels([name.replace('_', '\n') for name in weight_names], rotation=45)
        ax2.set_ylabel('æƒé‡å€¼')
    
    # ç³»ç»Ÿé…ç½®æ˜¾ç¤º
    config_info = system_stats.get('system_config', {})
    config_text = "æ™ºèƒ½ç³»ç»Ÿé…ç½®:\n"
    for key, value in config_info.items():
        config_text += f"â€¢ {key}: {value}\n"
    
    ax3.text(0.05, 0.95, config_text, transform=ax3.transAxes, fontsize=10,
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))
    ax3.set_title('ç³»ç»Ÿé…ç½®ä¿¡æ¯')
    ax3.axis('off')
    
    # å¹²é¢„ç»Ÿè®¡
    base_stats = system_stats.get('base_stats', {})
    intervention_info = f"""
    æ™ºèƒ½å¹²é¢„ç»Ÿè®¡:
    â€¢ æ€»å›åˆæ•°: {system_stats.get('episode_count', 0)}
    â€¢ è¿‘æœŸå¹²é¢„: {system_stats.get('recent_interventions', 0)}
    â€¢ ç”¨æˆ·è®¿é—®: {len(base_stats.get('user_visited_flags', []))}
    â€¢ å¼•å¯¼æ¿€æ´»: {base_stats.get('end_guidance_activated', False)}
    â€¢ å¼ºå¼•å¯¼: {base_stats.get('strong_guidance_activated', False)}
    """
    
    ax4.text(0.05, 0.95, intervention_info, transform=ax4.transAxes, fontsize=10,
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))
    ax4.set_title('æ™ºèƒ½å¹²é¢„ç»Ÿè®¡')
    ax4.axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, 'intelligent_system_analysis.png'), dpi=300, bbox_inches='tight')
    plt.close()


def generate_comprehensive_report(callback: Advanced6StageCallback,
                                stage_manager: Optimized6StageManager,
                                intelligent_system: IntelligentRewardSystem,
                                training_time: float, results_dir: str):
    """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
    report_path = os.path.join(results_dir, 'comprehensive_report.txt')
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=== 6é˜¶æ®µé«˜çº§è®­ç»ƒç³»ç»Ÿç»¼åˆæŠ¥å‘Š ===\n\n")
        f.write(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"è®­ç»ƒæ€»æ—¶é—´: {training_time/60:.1f}åˆ†é’Ÿ\n")
        f.write(f"æ€»è®­ç»ƒå›åˆ: {callback.episode_count}\n\n")
        
        # æ•´ä½“æ€§èƒ½
        if len(callback.success_history) > 0:
            f.write("=== æ•´ä½“æ€§èƒ½ ===\n")
            f.write(f"æ€»ä½“æˆåŠŸç‡: {np.mean(callback.success_history):.2%}\n")
            f.write(f"æœ€ç»ˆæˆåŠŸç‡: {np.mean(callback.success_history[-20:]):.2%}\n")
            f.write(f"å¹³å‡å¥–åŠ±: {np.mean(callback.reward_history):.0f}\n")
            f.write(f"æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(callback.reward_history[-20:]):.0f}\n\n")
        
        # å„é˜¶æ®µè¯¦æƒ…
        f.write("=== å„é˜¶æ®µè¯¦ç»†æ€§èƒ½ ===\n")
        if hasattr(callback, 'best_performance'):
            for stage_name, performance in callback.best_performance.items():
                stage_num = stage_name.split('_')[1]
                f.write(f"é˜¶æ®µ{stage_num}:\n")
                f.write(f"  è®­ç»ƒå›åˆ: {performance['episodes']}\n")
                f.write(f"  æˆåŠŸç‡: {performance['success_rate']:.2%}\n")
                f.write(f"  å¹³å‡å¥–åŠ±: {performance['avg_reward']:.0f}\n")
                f.write(f"  è®­ç»ƒæ—¶é—´: {performance['training_time']/60:.1f}åˆ†é’Ÿ\n\n")
        
        # æ™ºèƒ½ç³»ç»Ÿåˆ†æ
        system_stats = intelligent_system.get_system_stats()
        f.write("=== æ™ºèƒ½å¥–åŠ±ç³»ç»Ÿåˆ†æ ===\n")
        performance_stats = system_stats.get('performance_stats', {})
        f.write(f"ç³»ç»ŸæˆåŠŸç‡: {performance_stats.get('success_rate', 0):.2%}\n")
        f.write(f"å®Œæˆç‡: {performance_stats.get('completion_rate', 0):.2%}\n")
        f.write(f"æ€§èƒ½è¶‹åŠ¿: {performance_stats.get('performance_trend', 0):+.3f}\n")
        f.write(f"ç›‘æ§å›åˆ: {performance_stats.get('episodes_tracked', 0)}\n")
        f.write(f"æ™ºèƒ½å¹²é¢„: {system_stats.get('recent_interventions', 0)}æ¬¡\n\n")
        
        # ç³»ç»Ÿé…ç½®
        f.write("=== ç³»ç»Ÿé…ç½® ===\n")
        config_info = system_stats.get('system_config', {})
        for key, value in config_info.items():
            f.write(f"{key}: {value}\n")
        
        f.write(f"\n=== æ–‡ä»¶ä½ç½® ===\n")
        f.write(f"æŠ¥å‘Šç›®å½•: {os.path.abspath(results_dir)}\n")
        f.write(f"è®­ç»ƒæ›²çº¿: training_curves.png\n")
        f.write(f"é˜¶æ®µæ€§èƒ½: stage_performance.png\n")
        f.write(f"æ™ºèƒ½åˆ†æ: intelligent_system_analysis.png\n")
    
    print(f"ğŸ“„ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {report_path}")


def evaluate_final_performance(agent: DQN, env: UAVEnvironment, 
                             stage_manager: Optimized6StageManager,
                             num_episodes: int = 10) -> Dict[str, Any]:
    """
    è¯„ä¼°æœ€ç»ˆæ€§èƒ½
    """
    print(f"\nğŸ” === æœ€ç»ˆæ€§èƒ½è¯„ä¼° ({num_episodes}å›åˆ) === ğŸ”")
    
    evaluation_results = {
        'stage_results': {},
        'overall_performance': {}
    }
    
    # åœ¨æ¯ä¸ªé˜¶æ®µè¯„ä¼°æ€§èƒ½
    for stage in range(1, 7):
        stage_config = stage_manager.get_stage_config(stage)
        env.user_manager.set_user_positions(stage_config['user_positions'])
        
        stage_successes = 0
        stage_rewards = []
        
        print(f"\nğŸ“ è¯„ä¼°{stage_config['stage_name']}...")
        
        for episode in range(num_episodes):
            obs, _ = env.reset()
            total_reward = 0
            done = False
            step = 0
            
            while not done and step < 3000:
                action, _ = agent.predict(obs, deterministic=True)
                action = int(np.asarray(action).ravel()[0])  # ç¡®ä¿actionæ˜¯æ•´æ•°
                obs, reward, terminated, truncated, info = env.step(action)
                done = terminated or truncated
                total_reward += reward
                step += 1
            
            # æ£€æŸ¥æˆåŠŸ
            success = (info.get('users_visited', 0) >= len(stage_config['user_positions']) and 
                      info.get('reached_end', False))
            
            if success:
                stage_successes += 1
            stage_rewards.append(total_reward)
        
        # è®°å½•é˜¶æ®µç»“æœ
        stage_success_rate = stage_successes / num_episodes
        stage_avg_reward = np.mean(stage_rewards)
        
        evaluation_results['stage_results'][stage] = {
            'success_rate': stage_success_rate,
            'avg_reward': stage_avg_reward,
            'successes': stage_successes,
            'total_episodes': num_episodes
        }
        
        print(f"   æˆåŠŸç‡: {stage_success_rate:.2%}")
        print(f"   å¹³å‡å¥–åŠ±: {stage_avg_reward:.0f}")
    
    # è®¡ç®—æ•´ä½“æ€§èƒ½
    all_success_rates = [result['success_rate'] for result in evaluation_results['stage_results'].values()]
    all_avg_rewards = [result['avg_reward'] for result in evaluation_results['stage_results'].values()]
    
    evaluation_results['overall_performance'] = {
        'avg_success_rate': np.mean(all_success_rates),
        'min_success_rate': np.min(all_success_rates),
        'max_success_rate': np.max(all_success_rates),
        'avg_reward': np.mean(all_avg_rewards),
        'stages_above_30_percent': sum(1 for rate in all_success_rates if rate >= 0.3),
        'stages_above_50_percent': sum(1 for rate in all_success_rates if rate >= 0.5)
    }
    
    print(f"\nğŸ† === æ•´ä½“è¯„ä¼°ç»“æœ === ğŸ†")
    print(f"å¹³å‡æˆåŠŸç‡: {evaluation_results['overall_performance']['avg_success_rate']:.2%}")
    print(f"æˆåŠŸç‡èŒƒå›´: {evaluation_results['overall_performance']['min_success_rate']:.2%} - {evaluation_results['overall_performance']['max_success_rate']:.2%}")
    print(f"â‰¥30%æˆåŠŸç‡é˜¶æ®µ: {evaluation_results['overall_performance']['stages_above_30_percent']}/6")
    print(f"â‰¥50%æˆåŠŸç‡é˜¶æ®µ: {evaluation_results['overall_performance']['stages_above_50_percent']}/6")
    
    return evaluation_results


def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸŒŸ === å®Œæ•´6é˜¶æ®µé«˜çº§è®­ç»ƒç³»ç»Ÿ === ğŸŒŸ")
    print("é›†æˆåŠŸèƒ½:")
    print("  âœ… é«˜çº§ç»ˆç‚¹å¼•å¯¼æœºåˆ¶")
    print("  âœ… ä¼˜åŒ–6é˜¶æ®µè¯¾ç¨‹å­¦ä¹ ")
    print("  âœ… æ™ºèƒ½å¥–åŠ±ç³»ç»Ÿ")
    print("  âœ… åŠ¨æ€æƒé‡è°ƒæ•´")
    print("  âœ… ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¼•å¯¼")
    print("  âœ… å®Œæˆåº¦æ£€æµ‹")
    print("  âœ… æ™ºèƒ½æƒ©ç½šè°ƒæ•´")
    
    # è¯¢é—®è®­ç»ƒé…ç½®
    print(f"\nâš™ï¸ è®­ç»ƒé…ç½®é€‰æ‹©:")
    print("1. å¿«é€Ÿæµ‹è¯• (50,000æ­¥, ~10åˆ†é’Ÿ)")
    print("2. æ ‡å‡†è®­ç»ƒ (200,000æ­¥, ~40åˆ†é’Ÿ)")
    print("3. å®Œæ•´è®­ç»ƒ (500,000æ­¥, ~100åˆ†é’Ÿ)")
    print("4. è‡ªå®šä¹‰")
    
    choice = input("è¯·é€‰æ‹© (1-4): ").strip()
    
    if choice == '1':
        timesteps = 50000
    elif choice == '2':
        timesteps = 200000
    elif choice == '3':
        timesteps = 500000
    elif choice == '4':
        timesteps = int(input("è¯·è¾“å…¥è®­ç»ƒæ­¥æ•°: "))
    else:
        timesteps = 200000
        print("ä½¿ç”¨é»˜è®¤é…ç½®: 200,000æ­¥")
    
    # å¼€å§‹è®­ç»ƒ
    agent, env, callback = train_complete_6stage_system(timesteps)
    
    # æœ€ç»ˆæ€§èƒ½è¯„ä¼°
    evaluation_results = evaluate_final_performance(agent, env.unwrapped, callback.stage_manager)
    
    print(f"\nğŸ‰ === 6é˜¶æ®µé«˜çº§è®­ç»ƒç³»ç»Ÿå®Œæˆ === ğŸ‰")
    
    # åˆ¤æ–­è®­ç»ƒæ˜¯å¦æˆåŠŸ
    overall_perf = evaluation_results['overall_performance']
    if overall_perf['stages_above_30_percent'] >= 5:
        print("ğŸ† è®­ç»ƒéå¸¸æˆåŠŸï¼å¤§éƒ¨åˆ†é˜¶æ®µéƒ½è¾¾åˆ°äº†è‰¯å¥½çš„æ€§èƒ½ï¼")
    elif overall_perf['stages_above_30_percent'] >= 3:
        print("âœ… è®­ç»ƒåŸºæœ¬æˆåŠŸï¼å¤šæ•°é˜¶æ®µè¡¨ç°è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ä¼˜åŒ–ï¼")
    else:
        print("âš ï¸ è®­ç»ƒæ•ˆæœæœ‰é™ï¼Œå»ºè®®è°ƒæ•´å‚æ•°æˆ–å¢åŠ è®­ç»ƒæ—¶é—´ï¼")


if __name__ == '__main__':
    main()
