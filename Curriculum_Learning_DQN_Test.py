"""
è¯¾ç¨‹å­¦ä¹ DQNæµ‹è¯•
ä»ç®€å•åœºæ™¯é€æ­¥å¢åŠ å¤æ‚æ€§ï¼Œå¼•å¯¼UAVå­¦ä¼šæ­£ç¡®çš„è®¿é—®åºåˆ—
"""

import os
import sys
import time
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, Any, List

# è®¾ç½®matplotlib
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# ç¡®ä¿srcæ¨¡å—è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.join(current_dir, 'src')
sys.path.insert(0, src_path)

from stable_baselines3 import DQN
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.monitor import Monitor

from environment.uav_env import UAVEnvironment
from environment.curriculum_reward_config import (
    CurriculumRewardConfig, 
    CurriculumStageManager, 
    CurriculumRewardCalculator
)


class CurriculumLearningCallback(BaseCallback):
    """è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå›è°ƒ"""
    def __init__(self, stage_manager: CurriculumStageManager, verbose: int = 0):
        super().__init__(verbose)
        self.stage_manager = stage_manager
        self.episode_rewards = []
        self.episode_lengths = []
        self.episode_count = 0
        self.stage_history = []
        self.success_history = []

    def _on_step(self) -> bool:
        if len(self.locals.get('dones', [])) > 0:
            for i, done in enumerate(self.locals['dones']):
                if done:
                    infos = self.locals.get('infos', [])
                    if i < len(infos) and 'episode' in infos[i]:
                        ep_info = infos[i]['episode']
                        self.episode_rewards.append(float(ep_info['r']))
                        self.episode_lengths.append(int(ep_info['l']))
                        self.episode_count += 1
                        
                        # è¯¾ç¨‹å­¦ä¹ è¯„ä¼°
                        env = self.training_env.envs[0].unwrapped
                        
                        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç»ˆç‚¹ï¼ˆè·ç¦»åˆ¤å®šï¼‰
                        uav_pos = env.uav.get_position()
                        end_pos = env.end_position
                        distance_to_end = np.linalg.norm(uav_pos - end_pos)
                        reached_end = distance_to_end <= env.reward_calculator.config.end_position_tolerance
                        
                        episode_result = {
                            'total_reward': float(ep_info['r']),
                            'reached_end': reached_end,
                            'users_visited': 0
                        }
                        
                        if hasattr(env, 'reward_calculator'):
                            stats = env.reward_calculator.get_stats()
                            episode_result['users_visited'] = stats.get('users_visited', 0)
                        
                        # è¯„ä¼°æ˜¯å¦åº”è¯¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
                        should_advance = self.stage_manager.evaluate_stage_performance(episode_result)
                        
                        # è®°å½•å†å²
                        self.stage_history.append(self.stage_manager.current_stage)
                        self.success_history.append(episode_result['users_visited'] >= len(env.get_user_positions()) and episode_result['reached_end'])
                        
                        if should_advance:
                            self.stage_manager.advance_to_next_stage()
                            # æ›´æ–°ç¯å¢ƒä¸­çš„ç”¨æˆ·ä½ç½®
                            self._update_environment_for_new_stage()
                        
                        if self.verbose > 0 and self.episode_count % 5 == 0:
                            stage_info = self.stage_manager.get_current_stage_info()
                            print(f"Episode {self.episode_count}: reward={ep_info['r']:.2f}, length={ep_info['l']}")
                            print(f"  é˜¶æ®µ{stage_info['stage']}: æˆåŠŸç‡={stage_info['success_rate']:.2%} ({stage_info['successes']}/{stage_info['episodes']})")
                            print(f"  ç”¨æˆ·è®¿é—®: {episode_result['users_visited']}, åˆ°è¾¾ç»ˆç‚¹: {episode_result['reached_end']}")
        return True
    
    def _update_environment_for_new_stage(self):
        """ä¸ºæ–°é˜¶æ®µæ›´æ–°ç¯å¢ƒé…ç½®"""
        env = self.training_env.envs[0].unwrapped
        stage_config = self.stage_manager.get_stage_config(self.stage_manager.current_stage)
        
        # æ›´æ–°ç”¨æˆ·ä½ç½®
        new_user_positions = stage_config['user_positions']
        
        # ç¡®ä¿å§‹ç»ˆæœ‰2ä¸ªç”¨æˆ·ä½ç½®
        if len(new_user_positions) == 1:
            # é˜¶æ®µ1ï¼šæ·»åŠ è™šæ‹Ÿç”¨æˆ·
            extended_positions = np.array([
                new_user_positions[0],  # çœŸå®ç”¨æˆ·
                [200.0, 200.0, 0.0]     # è™šæ‹Ÿç”¨æˆ·ï¼ˆå¾ˆè¿œï¼‰
            ])
        else:
            extended_positions = new_user_positions
        
        env.user_manager.set_user_positions(extended_positions)
        
        print(f"ğŸ”„ ç¯å¢ƒå·²æ›´æ–°è‡³{stage_config['stage_name']}")
        print(f"   æ–°ç”¨æˆ·ä½ç½®: {new_user_positions[:, :2].tolist()}")
        if len(new_user_positions) == 1:
            print(f"   (æ·»åŠ è™šæ‹Ÿç”¨æˆ·ä½ç½®: [200, 200])")


def create_curriculum_learning_environment():
    """åˆ›å»ºè¯¾ç¨‹å­¦ä¹ UAVç¯å¢ƒ"""
    
    # è¯¾ç¨‹å­¦ä¹ é…ç½®
    reward_config = CurriculumRewardConfig(
        # === åŸºç¡€å¥–åŠ± ===
        w_throughput_base=100.0,
        w_movement_bonus=15.0,
        
        # === ç”¨æˆ·è®¿é—®å¥–åŠ± ===
        B_user_visit=2000.0,
        B_all_users_visited=3000.0,
        
        # === ç»ˆç‚¹å¥–åŠ± ===
        B_reach_end=2000.0,
        B_mission_complete=5000.0,
        
        # === å¼•å¯¼å¥–åŠ± ===
        w_user_approach=50.0,
        w_progress_bonus=30.0,
        
        # === æƒ©ç½š ===
        w_stagnation=3.0,
        w_oob=100.0,
        
        # === æœåŠ¡å‚æ•°ï¼ˆè°ƒæ•´ä¸ºæ›´å®½æ¾çš„æ¡ä»¶ï¼‰===
        user_service_radius=60.0,        # å¢å¤§æœåŠ¡åŠå¾„ï¼š40->60m
        close_to_user_threshold=80.0,    # å¢å¤§æ¥è¿‘é˜ˆå€¼ï¼š60->80m  
        end_position_tolerance=25.0,     # å¢å¤§ç»ˆç‚¹å®¹å¿ï¼š20->25m
        user_visit_time_threshold=0.8,   # å‡å°‘è®¿é—®æ—¶é—´è¦æ±‚ï¼š1.5->0.8s
        
        # === æ—¶é—´çº¦æŸ ===
        min_flight_time=200.0,
        max_flight_time=300.0,
        time_step=0.1
    )
    
    # åˆ›å»ºé˜¶æ®µç®¡ç†å™¨
    stage_manager = CurriculumStageManager()
    
    env = UAVEnvironment(
        env_size=(100, 100, 50),
        num_users=2,  # æœ€å¤§ç”¨æˆ·æ•°ï¼Œå®é™…ä½¿ç”¨æ•°ç”±è¯¾ç¨‹é˜¶æ®µå†³å®š
        num_antennas=8,
        start_position=(0, 0, 50),
        end_position=(80, 80, 50),
        flight_time=300.0,
        time_step=0.1,
        transmit_power=0.5,
        max_speed=30.0,
        min_speed=10.0,
        fixed_users=True,
        reward_config=None,
        seed=42
    )
    
    # è®¾ç½®è¯¾ç¨‹å­¦ä¹ å¥–åŠ±è®¡ç®—å™¨
    curriculum_calculator = CurriculumRewardCalculator(reward_config, stage_manager)
    env.set_reward_calculator(curriculum_calculator)
    
    # è®¾ç½®beamformingç­–ç•¥
    env.set_transmit_strategy(
        beamforming_method='mrt',
        power_strategy='proportional'
    )
    
    # åˆå§‹åŒ–ä¸ºé˜¶æ®µ1çš„ç”¨æˆ·ä½ç½®ï¼ˆéœ€è¦æ‰©å±•åˆ°2ä¸ªç”¨æˆ·ï¼‰
    stage1_config = stage_manager.get_stage_config(1)
    stage1_positions = stage1_config['user_positions']
    
    # å¯¹äºé˜¶æ®µ1ï¼ˆåªæœ‰1ä¸ªç”¨æˆ·ï¼‰ï¼Œéœ€è¦æ·»åŠ ä¸€ä¸ªè™šæ‹Ÿç”¨æˆ·ä½ç½®
    if len(stage1_positions) == 1:
        # æ·»åŠ ä¸€ä¸ªè¿œç¦»çš„è™šæ‹Ÿç”¨æˆ·ï¼Œä¸ä¼šå½±å“è®­ç»ƒ
        extended_positions = np.array([
            stage1_positions[0],  # çœŸå®ç”¨æˆ·
            [200.0, 200.0, 0.0]   # è™šæ‹Ÿç”¨æˆ·ï¼ˆå¾ˆè¿œï¼Œä¸ä¼šè¢«è®¿é—®ï¼‰
        ])
    else:
        extended_positions = stage1_positions
    
    env.user_manager.set_user_positions(extended_positions)
    
    return env, stage_manager


def train_curriculum_learning_dqn(env, stage_manager, total_timesteps=200000):
    """è®­ç»ƒè¯¾ç¨‹å­¦ä¹ DQN"""
    monitored_env = Monitor(env)
    
    # DQNé…ç½®ï¼šé€‚åˆè¯¾ç¨‹å­¦ä¹ 
    agent = DQN(
        policy='MlpPolicy',
        env=monitored_env,
        learning_rate=3e-4,
        gamma=0.99,
        batch_size=64,
        buffer_size=200000,
        exploration_fraction=0.6,  # å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨
        exploration_initial_eps=1.0,
        exploration_final_eps=0.05,
        verbose=1,
        seed=42,
        learning_starts=5000,
        train_freq=4,
        target_update_interval=2000,
        policy_kwargs=dict(
            net_arch=[256, 256, 128]
        )
    )
    
    callback = CurriculumLearningCallback(stage_manager, verbose=1)
    
    print("ğŸ“ å¼€å§‹è¯¾ç¨‹å­¦ä¹ DQNè®­ç»ƒ...")
    print("ğŸ“š è®­ç»ƒç­–ç•¥ï¼šä»ç®€å•åˆ°å¤æ‚ï¼Œé€æ­¥å­¦ä¹ ")
    print("ğŸ¯ é˜¶æ®µ1ï¼šå•ç”¨æˆ· â†’ é˜¶æ®µ2ï¼šåŒç”¨æˆ·è¿‘è·ç¦» â†’ é˜¶æ®µ3ï¼šåŒç”¨æˆ·ä¸­è·ç¦» â†’ é˜¶æ®µ4ï¼šå®Œæ•´åœºæ™¯")
    
    # æ˜¾ç¤ºåˆå§‹é˜¶æ®µä¿¡æ¯
    stage1_config = stage_manager.get_stage_config(1)
    print(f"\nğŸš€ å¼€å§‹ {stage1_config['stage_name']}")
    print(f"   ç›®æ ‡: {stage1_config['description']}")
    print(f"   ç”¨æˆ·ä½ç½®: {stage1_config['user_positions'][:, :2].tolist()}")
    
    start_time = time.time()
    
    agent.learn(
        total_timesteps=total_timesteps,
        callback=callback,
        progress_bar=True
    )
    
    training_time = time.time() - start_time
    
    print(f"âœ… è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå®Œæˆ! è€—æ—¶: {training_time:.1f}ç§’")
    print(f"ğŸ“ˆ æ€»å›åˆæ•°: {callback.episode_count}")
    print(f"ğŸ“ æœ€ç»ˆé˜¶æ®µ: {stage_manager.current_stage}/4")
    
    if callback.episode_rewards:
        print(f"å¹³å‡å¥–åŠ±: {np.mean(callback.episode_rewards):.2f}")
        print(f"æœ€å10å›åˆå¹³å‡å¥–åŠ±: {np.mean(callback.episode_rewards[-10:]):.2f}")
        
        # è®¡ç®—å„é˜¶æ®µæˆåŠŸç‡
        final_success_rate = np.mean(callback.success_history[-20:]) if len(callback.success_history) >= 20 else 0
        print(f"æœ€è¿‘20å›åˆæˆåŠŸç‡: {final_success_rate:.2%}")
    
    return agent, callback, monitored_env


def evaluate_curriculum_trajectory(agent, env, stage_manager, deterministic=True):
    """è¯„ä¼°è¯¾ç¨‹å­¦ä¹ è½¨è¿¹ï¼ˆåœ¨æœ€ç»ˆé˜¶æ®µï¼‰"""
    
    # ç¡®ä¿åœ¨æœ€ç»ˆé˜¶æ®µï¼ˆé˜¶æ®µ4ï¼‰è¯„ä¼°
    stage4_config = stage_manager.get_stage_config(4)
    env.unwrapped.user_manager.set_user_positions(stage4_config['user_positions'])
    
    obs, _ = env.reset()
    
    trajectory = []
    rewards = []
    reward_breakdowns = []
    throughputs = []
    actions = []
    
    done = False
    step = 0
    
    print(f"ğŸ” åœ¨æœ€ç»ˆé˜¶æ®µè¯„ä¼°è¯¾ç¨‹å­¦ä¹ æ•ˆæœ...")
    print(f"   ç”¨æˆ·ä½ç½®: {stage4_config['user_positions'][:, :2].tolist()}")
    
    while not done and step < 3000:
        action, _ = agent.predict(obs, deterministic=deterministic)
        action = int(np.asarray(action).ravel()[0])
        
        obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        
        trajectory.append(env.unwrapped.uav.get_position().copy())
        rewards.append(reward)
        throughputs.append(info.get('throughput', 0.0))
        actions.append(action)
        
        # è®°å½•å¥–åŠ±åˆ†è§£
        if hasattr(env.unwrapped, '_last_reward_breakdown'):
            reward_breakdowns.append(env.unwrapped._last_reward_breakdown.copy())
        else:
            reward_breakdowns.append({})
        
        step += 1
    
    trajectory = np.array(trajectory)
    
    return {
        'trajectory': trajectory,
        'rewards': rewards,
        'reward_breakdowns': reward_breakdowns,
        'throughputs': throughputs,
        'actions': actions,
        'total_reward': sum(rewards),
        'total_throughput': sum(throughputs),
        'steps': len(trajectory),
        'reached_end': terminated,
        'final_position': trajectory[-1] if len(trajectory) > 0 else None,
        'target_position': env.unwrapped.end_position
    }


def plot_curriculum_analysis(result, env, callback, stage_manager):
    """ç»˜åˆ¶è¯¾ç¨‹å­¦ä¹ åˆ†æå›¾è¡¨"""
    
    plt.figure(figsize=(20, 16))
    
    # === 1. è®­ç»ƒæ”¶æ•›æ›²çº¿ï¼ˆæŒ‰é˜¶æ®µç€è‰²ï¼‰===
    plt.subplot(4, 4, 1)
    episode_rewards = callback.episode_rewards
    stage_history = callback.stage_history
    
    # æŒ‰é˜¶æ®µç€è‰²ç»˜åˆ¶
    stage_colors = ['blue', 'green', 'orange', 'red']
    current_stage = 1
    start_idx = 0
    
    for i, stage in enumerate(stage_history):
        if stage != current_stage or i == len(stage_history) - 1:
            end_idx = i if stage != current_stage else i + 1
            if end_idx > start_idx:
                episodes = range(start_idx, end_idx)
                rewards = episode_rewards[start_idx:end_idx]
                plt.plot(episodes, rewards, color=stage_colors[current_stage-1], 
                        alpha=0.7, linewidth=1, label=f'é˜¶æ®µ{current_stage}')
            current_stage = stage
            start_idx = i
    
    plt.xlabel('Episode')
    plt.ylabel('Episode Reward')
    plt.title('ğŸ“š è¯¾ç¨‹å­¦ä¹ è®­ç»ƒæ”¶æ•›ï¼ˆæŒ‰é˜¶æ®µï¼‰')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # === 2. é˜¶æ®µè½¬æ¢å†å² ===
    plt.subplot(4, 4, 2)
    episodes = range(len(stage_history))
    plt.plot(episodes, stage_history, 'b-', linewidth=2, marker='o', markersize=3)
    plt.ylim(0.5, 4.5)
    plt.yticks([1, 2, 3, 4], ['é˜¶æ®µ1\nå•ç”¨æˆ·', 'é˜¶æ®µ2\nåŒç”¨æˆ·è¿‘', 'é˜¶æ®µ3\nåŒç”¨æˆ·ä¸­', 'é˜¶æ®µ4\nå®Œæ•´åœºæ™¯'])
    plt.xlabel('Episode')
    plt.ylabel('è¯¾ç¨‹é˜¶æ®µ')
    plt.title('ğŸ“ˆ è¯¾ç¨‹é˜¶æ®µæ¼”è¿›')
    plt.grid(True, alpha=0.3)
    
    # === 3. æˆåŠŸç‡å†å² ===
    plt.subplot(4, 4, 3)
    if callback.success_history:
        # è®¡ç®—æ»‘åŠ¨æˆåŠŸç‡
        window_size = 10
        success_rates = []
        for i in range(len(callback.success_history)):
            start = max(0, i - window_size + 1)
            window_successes = callback.success_history[start:i+1]
            success_rates.append(np.mean(window_successes))
        
        episodes = range(len(success_rates))
        plt.plot(episodes, success_rates, 'g-', linewidth=2)
        plt.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='ç›®æ ‡æˆåŠŸç‡')
        plt.ylim(0, 1)
        plt.xlabel('Episode')
        plt.ylabel('æˆåŠŸç‡ï¼ˆæ»‘åŠ¨çª—å£ï¼‰')
        plt.title('ğŸ“Š å­¦ä¹ æˆåŠŸç‡æ¼”è¿›')
        plt.legend()
        plt.grid(True, alpha=0.3)
    
    # === 4. æœ€ç»ˆè½¨è¿¹å›¾ ===
    plt.subplot(4, 4, 4)
    trajectory = result['trajectory']
    if len(trajectory) > 0:
        # ç»˜åˆ¶è½¨è¿¹
        plt.plot(trajectory[:, 0], trajectory[:, 1], 'b-', linewidth=3, alpha=0.8, label='æœ€ç»ˆè¯„ä¼°è½¨è¿¹')
        
        # ç¯å¢ƒå…ƒç´ 
        env_unwrapped = env.unwrapped
        plt.scatter(*env_unwrapped.start_position[:2], c='green', s=200, marker='o', 
                   label='èµ·ç‚¹', zorder=10, edgecolors='black', linewidth=2)
        plt.scatter(*env_unwrapped.end_position[:2], c='red', s=300, marker='*', 
                   label='ç»ˆç‚¹', zorder=10, edgecolors='black', linewidth=2)
        
        # æœ€ç»ˆé˜¶æ®µç”¨æˆ·ä½ç½®
        user_positions = env_unwrapped.get_user_positions()
        for i, user_pos in enumerate(user_positions):
            plt.scatter(user_pos[0], user_pos[1], c='purple', s=150, marker='x',
                       label=f'ç”¨æˆ·{i+1}' if i == 0 else "", zorder=8)
            
            # æœåŠ¡åœ†åœˆ
            service_circle = plt.Circle((user_pos[0], user_pos[1]), 40,
                                      fill=False, color='purple', linestyle='--',
                                      linewidth=2, alpha=0.6)
            plt.gca().add_patch(service_circle)
        
        # ç»ˆç‚¹å®¹å¿åœ†åœˆ
        tolerance_circle = plt.Circle((80, 80), 20,
                                    fill=False, color='red', linestyle='-',
                                    linewidth=2, alpha=0.8)
        plt.gca().add_patch(tolerance_circle)
        
    plt.xlim(-5, 105)
    plt.ylim(-5, 105)
    plt.xlabel('X (m)')
    plt.ylabel('Y (m)')
    plt.title('ğŸ¯ æœ€ç»ˆè¯„ä¼°è½¨è¿¹ï¼ˆé˜¶æ®µ4ï¼‰')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.gca().set_aspect('equal')
    
    # === 5. å¥–åŠ±åˆ†è§£åˆ†æ ===
    plt.subplot(4, 4, 5)
    if result['reward_breakdowns']:
        approach_rewards = [rb.get('user_approach', 0) + rb.get('end_approach', 0) for rb in result['reward_breakdowns']]
        visit_bonuses = [rb.get('user_visit_bonus', 0) for rb in result['reward_breakdowns']]
        progress_bonuses = [rb.get('progress_bonus', 0) for rb in result['reward_breakdowns']]
        
        steps = range(len(approach_rewards))
        plt.plot(steps, approach_rewards, label='æ¥è¿‘å¥–åŠ±', alpha=0.8)
        plt.plot(steps, visit_bonuses, label='è®¿é—®å¥–åŠ±', alpha=0.8)
        plt.plot(steps, progress_bonuses, label='è¿›å±•å¥–åŠ±', alpha=0.8)
        
        plt.xlabel('æ—¶é—´æ­¥')
        plt.ylabel('å¥–åŠ±å€¼')
        plt.title('ğŸ è¯¾ç¨‹å­¦ä¹ å¥–åŠ±åˆ†è§£')
        plt.legend()
        plt.grid(True, alpha=0.3)
    
    # === 6. ç´¯ç§¯å¥–åŠ± ===
    plt.subplot(4, 4, 6)
    cumulative_rewards = np.cumsum(result['rewards'])
    plt.plot(cumulative_rewards, 'cyan', linewidth=2)
    plt.xlabel('æ—¶é—´æ­¥')
    plt.ylabel('ç´¯ç§¯å¥–åŠ±')
    plt.title('ğŸ“ˆ æœ€ç»ˆè¯„ä¼°ç´¯ç§¯å¥–åŠ±')
    plt.grid(True, alpha=0.3)
    
    # === 7. åŠ¨ä½œåˆ†å¸ƒ ===
    plt.subplot(4, 4, 7)
    actions = result['actions']
    action_names = ['ä¸œ', 'å—', 'è¥¿', 'åŒ—', 'æ‚¬åœ']
    action_counts = [actions.count(i) for i in range(5)]
    colors = ['red', 'blue', 'green', 'orange', 'purple']
    bars = plt.bar(action_names, action_counts, alpha=0.7, color=colors)
    plt.xlabel('åŠ¨ä½œç±»å‹')
    plt.ylabel('æ¬¡æ•°')
    plt.title('ğŸ® æœ€ç»ˆè¯„ä¼°åŠ¨ä½œåˆ†å¸ƒ')
    plt.grid(True, alpha=0.3)
    
    for bar, count in zip(bars, action_counts):
        height = bar.get_height()
        if count > 0:
            plt.text(bar.get_x() + bar.get_width()/2., height + max(action_counts)*0.01,
                    f'{count}', ha='center', va='bottom', fontweight='bold')
    
    # === 8. è¯¾ç¨‹å­¦ä¹ ç»Ÿè®¡æ‘˜è¦ ===
    plt.subplot(4, 4, 8)
    plt.axis('off')
    
    # åˆ†ææœ€ç»ˆç»“æœ
    final_stats = {}
    if hasattr(env.unwrapped, 'reward_calculator'):
        final_stats = env.unwrapped.reward_calculator.get_stats()
    
    visited_users = final_stats.get('user_visited_flags', [])
    visit_order = final_stats.get('user_visit_order', [])
    users_visited = final_stats.get('users_visited', 0)
    
    final_pos = result['final_position']
    target_pos = result['target_position']
    final_distance = np.linalg.norm(final_pos - target_pos) if final_pos is not None else float('inf')
    
    # è®¡ç®—æ•´ä½“æˆåŠŸç‡
    total_success_rate = np.mean(callback.success_history) if callback.success_history else 0
    final_success_rate = np.mean(callback.success_history[-20:]) if len(callback.success_history) >= 20 else 0
    
    summary_text = f"""
ğŸ“š è¯¾ç¨‹å­¦ä¹ è®­ç»ƒæŠ¥å‘Š

ğŸ“ è®­ç»ƒç»Ÿè®¡:
â€¢ æ€»å›åˆæ•°: {callback.episode_count}
â€¢ æœ€ç»ˆé˜¶æ®µ: {stage_manager.current_stage}/4
â€¢ æ•´ä½“æˆåŠŸç‡: {total_success_rate:.2%}
â€¢ æœ€ç»ˆæˆåŠŸç‡: {final_success_rate:.2%}

ğŸ¯ æœ€ç»ˆè¯„ä¼°ç»“æœ:
â€¢ è®¿é—®ç”¨æˆ·: {visited_users}
â€¢ è®¿é—®é¡ºåº: {visit_order}
â€¢ åˆ°è¾¾ç»ˆç‚¹: {'æ˜¯' if result['reached_end'] else 'å¦'}
â€¢ æœ€ç»ˆè·ç¦»: {final_distance:.1f}m
â€¢ æ€»å¥–åŠ±: {result['total_reward']:.0f}

ğŸ† è¯¾ç¨‹å­¦ä¹ æ•ˆæœ:
{
'ğŸ‰ å®Œç¾æˆåŠŸï¼å®Œå…¨æŒæ¡ä»»åŠ¡!' if users_visited == 2 and result['reached_end']
else f'ğŸ”¶ éƒ¨åˆ†æˆåŠŸï¼šè®¿é—®{users_visited}/2ç”¨æˆ·' + ('ï¼Œåˆ°è¾¾ç»ˆç‚¹' if result['reached_end'] else 'ï¼Œæœªåˆ°ç»ˆç‚¹')
if users_visited > 0 or result['reached_end']
else 'âŒ è¯¾ç¨‹å­¦ä¹ éœ€è¦è°ƒæ•´'
}

ğŸ’¡ å­¦ä¹ è´¨é‡:
{
'ğŸŒŸ è¯¾ç¨‹å­¦ä¹ ç­–ç•¥æœ‰æ•ˆ!' if final_success_rate > 0.3
else 'ğŸ“š éœ€è¦æ›´å¤šè®­ç»ƒæˆ–è°ƒæ•´è¯¾ç¨‹'
}
    """
    
    plt.text(0.05, 0.95, summary_text, transform=plt.gca().transAxes,
            fontsize=8, verticalalignment='top',
            bbox=dict(boxstyle="round,pad=0.5", facecolor="lightcyan", alpha=0.8))
    
    plt.tight_layout()
    plt.show()


def main():
    print("ğŸ“š === è¯¾ç¨‹å­¦ä¹ DQNè®­ç»ƒ - ä»ç®€å•åˆ°å¤æ‚ === ğŸ“š")
    print("ç­–ç•¥ï¼šé€æ­¥å¢åŠ ä»»åŠ¡å¤æ‚æ€§ï¼Œå¼•å¯¼UAVå­¦ä¼šæ­£ç¡®çš„è®¿é—®åºåˆ—")
    print("ç›®æ ‡ï¼šé€šè¿‡æ¸è¿›å¼å­¦ä¹ å…‹æœç›´æ¥è®­ç»ƒçš„å›°éš¾\n")
    
    # 1. åˆ›å»ºè¯¾ç¨‹å­¦ä¹ ç¯å¢ƒ
    env, stage_manager = create_curriculum_learning_environment()
    print("âœ… è¯¾ç¨‹å­¦ä¹ ç¯å¢ƒåˆ›å»ºå®Œæˆ")
    
    # 2. è®­ç»ƒè¯¾ç¨‹å­¦ä¹ DQN
    agent, callback, monitored_env = train_curriculum_learning_dqn(
        env, stage_manager, total_timesteps=200000
    )
    
    # 3. è¯„ä¼°æœ€ç»ˆæ•ˆæœ
    print("\nğŸ“Š åœ¨æœ€ç»ˆé˜¶æ®µè¯„ä¼°è¯¾ç¨‹å­¦ä¹ æ•ˆæœ...")
    result = evaluate_curriculum_trajectory(agent, monitored_env, stage_manager, deterministic=True)
    
    print(f"\nğŸ“š === è¯¾ç¨‹å­¦ä¹ æœ€ç»ˆè¯„ä¼°ç»“æœ === ğŸ“š")
    print(f"æ€»å¥–åŠ±: {result['total_reward']:.2f}")
    print(f"æ€»ååé‡: {result['total_throughput']:.2f}")
    print(f"æ­¥æ•°: {result['steps']}")
    print(f"åˆ°è¾¾ç»ˆç‚¹: {result['reached_end']}")
    
    if result['final_position'] is not None:
        distance_to_target = np.linalg.norm(result['final_position'] - result['target_position'])
        print(f"æœ€ç»ˆè·ç¦»: {distance_to_target:.2f}m")
        print(f"å®¹å¿åº¦: 20.0m")
        
        # åˆ†æè¯¾ç¨‹å­¦ä¹ æ•ˆæœ
        if hasattr(monitored_env.unwrapped, 'reward_calculator'):
            final_stats = monitored_env.unwrapped.reward_calculator.get_stats()
            visited_users = final_stats.get('user_visited_flags', [])
            visit_order = final_stats.get('user_visit_order', [])
            users_visited = final_stats.get('users_visited', 0)
            
            print(f"è®¿é—®ç”¨æˆ·: {visited_users}")
            print(f"è®¿é—®é¡ºåº: {visit_order}")
            print(f"è®¿é—®å®Œæˆåº¦: {users_visited}/2")
            
            # æœ€ç»ˆè¯„ä¼°
            if users_visited == 2 and result['reached_end']:
                print("ğŸ‰ è¯¾ç¨‹å­¦ä¹ å®Œå…¨æˆåŠŸï¼UAVå­¦ä¼šäº†å®Œæ•´çš„ä»»åŠ¡åºåˆ—ï¼")
                print("âœ¨ ä»ç®€å•åˆ°å¤æ‚çš„å­¦ä¹ ç­–ç•¥è¯æ˜æœ‰æ•ˆï¼")
            elif users_visited >= 1:
                print(f"ğŸ”¶ è¯¾ç¨‹å­¦ä¹ éƒ¨åˆ†æˆåŠŸï¼šå­¦ä¼šäº†è®¿é—®{users_visited}ä¸ªç”¨æˆ·")
                print("ğŸ’ª è¯´æ˜è¯¾ç¨‹å­¦ä¹ ç­–ç•¥å¼€å§‹å¥æ•ˆï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜")
            else:
                print("âŒ è¯¾ç¨‹å­¦ä¹ æ•ˆæœæœ‰é™ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è¯¾ç¨‹è®¾è®¡")
                print("ğŸ’¡ å»ºè®®ï¼šç®€åŒ–åˆå§‹é˜¶æ®µæˆ–å¢åŠ æ›´å¤šæ¸è¿›æ­¥éª¤")
            
            # æ•´ä½“æˆåŠŸç‡åˆ†æ
            final_success_rate = np.mean(callback.success_history[-20:]) if len(callback.success_history) >= 20 else 0
            print(f"æœ€è¿‘20å›åˆæˆåŠŸç‡: {final_success_rate:.2%}")
            
            if final_success_rate > 0.5:
                print("ğŸŒŸ è¯¾ç¨‹å­¦ä¹ æ˜¾è‘—æå‡äº†ä»»åŠ¡å®Œæˆèƒ½åŠ›ï¼")
            elif final_success_rate > 0.2:
                print("ğŸ“ˆ è¯¾ç¨‹å­¦ä¹ æœ‰ä¸€å®šæ•ˆæœï¼Œä½†è¿˜æœ‰æå‡ç©ºé—´")
            else:
                print("ğŸ“š è¯¾ç¨‹å­¦ä¹ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–è®¾è®¡")
    
    # 4. ç»˜åˆ¶è¯¾ç¨‹å­¦ä¹ åˆ†æ
    plot_curriculum_analysis(result, monitored_env, callback, stage_manager)
    
    print(f"\nğŸ“š === è¯¾ç¨‹å­¦ä¹ DQNè®­ç»ƒå®Œæˆ === ğŸ“š")
    print("ğŸ¯ è¯¾ç¨‹å­¦ä¹ é€šè¿‡é€æ­¥å¢åŠ å¤æ‚æ€§ï¼Œä¸ºè§£å†³åºåˆ—è®¿é—®é—®é¢˜æä¾›äº†æ–°æ€è·¯")


if __name__ == '__main__':
    main()
