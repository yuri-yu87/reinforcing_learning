# UAV-RLç³»ç»Ÿæ¶æ„æŒ‡å—

## ğŸ—ï¸ ç³»ç»Ÿæ¦‚è§ˆ

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜UAVå¼ºåŒ–å­¦ä¹ ç³»ç»Ÿçš„åˆ†å±‚æ¶æ„ã€èŒè´£åˆ†å·¥ã€æ¥å£è®¾è®¡å’ŒçŠ¶æ€ç®¡ç†æœºåˆ¶ã€‚

## ğŸ“‹ æ¶æ„å±‚æ¬¡

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   åº”ç”¨å±‚ (Application Layer)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  è®­ç»ƒè„šæœ¬       â”‚  â”‚  è¯„ä¼°è„šæœ¬       â”‚  â”‚  å¯è§†åŒ–è„šæœ¬ â”‚ â”‚
â”‚  â”‚  train_*.py     â”‚  â”‚  evaluate_*.py  â”‚  â”‚  plot_*.py  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  è®­ç»ƒå±‚ (Training Layer)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Trainer        â”‚  â”‚  TrainingConfig â”‚  â”‚  Callbacks  â”‚ â”‚
â”‚  â”‚  (è®­ç»ƒç®¡ç†å™¨)    â”‚  â”‚  (é…ç½®ç®¡ç†)     â”‚  â”‚  (å›è°ƒç›‘æ§) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  æ™ºèƒ½ä½“å±‚ (Agent Layer)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  RL Agents      â”‚  â”‚  BaselineAgent  â”‚  â”‚  BaseAgent  â”‚ â”‚
â”‚  â”‚  (PPO/SAC/DQN)  â”‚  â”‚  (ç¡®å®šæ€§ç­–ç•¥)   â”‚  â”‚  (æŠ½è±¡åŸºç±») â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ç¯å¢ƒå±‚ (Environment Layer)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  UAVEnvironment â”‚  â”‚  UAV            â”‚  â”‚  UserManagerâ”‚ â”‚
â”‚  â”‚  (ä¸»ç¯å¢ƒ)       â”‚  â”‚  (UAVå®ä½“)      â”‚  â”‚  (ç”¨æˆ·ç®¡ç†) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  å·¥å…·å±‚ (Utility Layer)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ChannelModel   â”‚  â”‚  SignalProcessorâ”‚  â”‚  Math Utils â”‚ â”‚
â”‚  â”‚  (ä¿¡é“æ¨¡å‹)     â”‚  â”‚  (ä¿¡å·å¤„ç†)     â”‚  â”‚  (æ•°å­¦å·¥å…·) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ å„å±‚èŒè´£è¯¦è§£

### 1. åº”ç”¨å±‚ (Application Layer)

**æ ¸å¿ƒèŒè´£**: ç”¨æˆ·äº¤äº’ã€æµç¨‹æ§åˆ¶ã€ç»“æœå±•ç¤º

**ç»„ä»¶èŒè´£**:
- **è®­ç»ƒè„šæœ¬** (`train_*.py`): é…ç½®è®­ç»ƒå‚æ•°ï¼Œå¯åŠ¨è®­ç»ƒæµç¨‹
- **è¯„ä¼°è„šæœ¬** (`evaluate_*.py`): è¿è¡Œæ€§èƒ½è¯„ä¼°ï¼Œç”Ÿæˆç»“æœæŠ¥å‘Š
- **å¯è§†åŒ–è„šæœ¬** (`generate_*.py`): åˆ›å»ºå›¾è¡¨ã€åŠ¨ç”»å’Œåˆ†ææŠ¥å‘Š

**è®¾è®¡åŸåˆ™**:
- ä¸ç›´æ¥è°ƒç”¨åº•å±‚ç»„ä»¶
- é€šè¿‡Traineræˆ–Agentè®¿é—®ç³»ç»ŸåŠŸèƒ½
- è´Ÿè´£æ•´ä½“ä¸šåŠ¡é€»è¾‘åè°ƒ

**ç¤ºä¾‹æ¥å£**:
```python
# è®­ç»ƒè„šæœ¬ç¤ºä¾‹
def main():
    config = TrainingConfig(agent_type='benchmark_1')
    trainer = Trainer(config)
    trainer.setup_environment()
    trainer.setup_agent()
    results = trainer.train()
```

### 2. è®­ç»ƒå±‚ (Training Layer)

**æ ¸å¿ƒèŒè´£**: è®­ç»ƒç®¡ç†ã€é…ç½®æ§åˆ¶ã€è¿›åº¦ç›‘æ§

**ç»„ä»¶èŒè´£**:
- **Trainer**: ç®¡ç†è®­ç»ƒå¾ªç¯ã€ç¯å¢ƒè®¾ç½®ã€Agentåè°ƒ
- **TrainingConfig**: é›†ä¸­ç®¡ç†æ‰€æœ‰é…ç½®å‚æ•°
- **TrainingCallback**: ç›‘æ§è®­ç»ƒè¿›åº¦ã€è®°å½•æŒ‡æ ‡

**è®¾è®¡åŸåˆ™**:
- æä¾›é«˜çº§è®­ç»ƒæ¥å£
- éš”ç¦»å…·ä½“ç®—æ³•ç»†èŠ‚
- æ”¯æŒå¤šç§Agentç±»å‹

**å…³é”®æ¥å£**:
```python
class Trainer:
    def setup_environment() -> UAVEnvironment
    def setup_agent() -> None
    def train() -> Dict[str, Any]
    def evaluate() -> Dict[str, Any]
```

### 3. æ™ºèƒ½ä½“å±‚ (Agent Layer)

**æ ¸å¿ƒèŒè´£**: å†³ç­–é€»è¾‘ã€ç­–ç•¥å®ç°ã€åŠ¨ä½œé€‰æ‹©

**ç»„ä»¶èŒè´£**:
- **BaseAgent**: å®šä¹‰ç»Ÿä¸€çš„Agentæ¥å£
- **RL Agents** (PPO/SAC/DQN): å®ç°å¼ºåŒ–å­¦ä¹ ç®—æ³•
- **BaselineAgent**: å®ç°ç¡®å®šæ€§ç­–ç•¥ï¼ˆç›´çº¿ã€è´ªå©ªã€ç¯å½¢ç­‰ï¼‰
- **Benchmark Agents**: ä¸“é—¨çš„åŸºå‡†æµ‹è¯•Agent

**è®¾è®¡åŸåˆ™**:
- ç»Ÿä¸€çš„æ¥å£è®¾è®¡ (select_action, update)
- æ”¯æŒå¤šç§å†³ç­–ç­–ç•¥
- ä¸ç¯å¢ƒè§£è€¦ï¼Œä¸“æ³¨å†³ç­–é€»è¾‘

**æ ¸å¿ƒæ¥å£**:
```python
class BaseAgent(ABC):
    @abstractmethod
    def select_action(self, observation: np.ndarray, deterministic: bool = False) -> np.ndarray
    
    @abstractmethod
    def update(self, batch: Dict[str, np.ndarray]) -> Dict[str, float]
```

**ç‰¹åŒ–Agent**:
```python
class Benchmark1Agent(BaselineAgent):
    """ç›´çº¿è½¨è¿¹ + ä¼˜åŒ–æ³¢æŸæˆå½¢"""
    
class Benchmark2Agent(BaselineAgent):
    """ç›´çº¿è½¨è¿¹ + éšæœºæ³¢æŸæˆå½¢"""
```

### 4. ç¯å¢ƒå±‚ (Environment Layer)

**æ ¸å¿ƒèŒè´£**: ç¯å¢ƒæ¨¡æ‹Ÿã€çŠ¶æ€ç®¡ç†ã€å¥–åŠ±è®¡ç®—

**ç»„ä»¶èŒè´£**:
- **UAVEnvironment**: ä¸»ç¯å¢ƒç±»ï¼Œå®ç°OpenAI Gymæ¥å£
- **UAV**: UAVç‰©ç†å®ä½“ï¼Œç®¡ç†ä½ç½®ã€é€Ÿåº¦ã€è½¨è¿¹
- **UserManager**: åœ°é¢ç”¨æˆ·ç®¡ç†ï¼Œä½ç½®åˆ†é…ã€é€šä¿¡è®°å½•

**è®¾è®¡åŸåˆ™**:
- ä¸“æ³¨ç¯å¢ƒæ¨¡æ‹Ÿï¼Œä¸åŒ…å«å†³ç­–é€»è¾‘
- æä¾›æ ‡å‡†çš„RLç¯å¢ƒæ¥å£
- ç®¡ç†ç‰©ç†çŠ¶æ€å’Œçº¦æŸ

**æ ¸å¿ƒæ¥å£**:
```python
class UAVEnvironment(gym.Env):
    def reset(self, seed: Optional[int] = None) -> Tuple[np.ndarray, Dict]
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]
    def _calculate_reward(self, throughput: float) -> float
    def _get_observation() -> np.ndarray
```

**çŠ¶æ€ç®¡ç†**:
- UAVä½ç½®ã€é€Ÿåº¦ã€è½¨è¿¹å†å²
- ç”¨æˆ·ä½ç½®ã€é€šä¿¡è´¨é‡ã€ååé‡å†å²
- æ—¶é—´ç®¡ç†ã€ä»»åŠ¡è¿›åº¦ã€ç»ˆæ­¢æ¡ä»¶

### 5. å·¥å…·å±‚ (Utility Layer)

**æ ¸å¿ƒèŒè´£**: åŸºç¡€åŠŸèƒ½ã€æ•°å­¦è®¡ç®—ã€ä¿¡å·å¤„ç†

**ç»„ä»¶èŒè´£**:
- **ChannelModel**: ä¿¡é“å»ºæ¨¡ã€è·¯å¾„æŸè€—è®¡ç®—
- **SignalProcessor**: æ³¢æŸæˆå½¢ã€åŠŸç‡åˆ†é…ã€ååé‡è®¡ç®—
- **æ•°å­¦å·¥å…·**: å‡ ä½•è®¡ç®—ã€ç»Ÿè®¡å‡½æ•°ã€ä¼˜åŒ–ç®—æ³•

**è®¾è®¡åŸåˆ™**:
- æä¾›çº¯å‡½æ•°å¼æ¥å£
- ä¸ä¾èµ–ä¸Šå±‚ç»„ä»¶
- å¯ç‹¬ç«‹æµ‹è¯•å’ŒéªŒè¯

## ğŸ”„ æ¥å£è§„èŒƒ

### æ ‡å‡†æ•°æ®æµ

```
Application â†’ Trainer â†’ Agent â†’ Environment â†’ Utils
     â†“            â†“         â†“          â†“          â†“
  ç»“æœå±•ç¤º    è®­ç»ƒç®¡ç†   åŠ¨ä½œé€‰æ‹©   çŠ¶æ€æ›´æ–°   åŸºç¡€è®¡ç®—
```

### å…³é”®æ¥å£

#### 1. Agent-Environmentæ¥å£
```python
# Agentå‘Environmentå‘é€åŠ¨ä½œ
action = agent.select_action(observation)
# Environmentè¿”å›æ–°çŠ¶æ€
observation, reward, terminated, truncated, info = env.step(action)
```

#### 2. Environment-Utilsæ¥å£
```python
# Environmentè°ƒç”¨Utilsè¿›è¡Œè®¡ç®—
throughput = signal_processor.calculate_throughput(channel_coeffs, beamforming_vectors)
path_loss = channel_model.calculate_path_loss(distance)
```

#### 3. Trainer-Agentæ¥å£
```python
# Trainerç®¡ç†Agentè®­ç»ƒ
trainer.setup_agent()
results = trainer.train()
metrics = trainer.evaluate()
```

## ğŸ“Š çŠ¶æ€ç®¡ç†æœºåˆ¶

### çŠ¶æ€å±‚æ¬¡ç»“æ„

```
å…¨å±€çŠ¶æ€ (Global State)
â”œâ”€â”€ ç¯å¢ƒçŠ¶æ€ (Environment State)
â”‚   â”œâ”€â”€ UAVçŠ¶æ€ (UAV State)
â”‚   â”‚   â”œâ”€â”€ ä½ç½® (position)
â”‚   â”‚   â”œâ”€â”€ é€Ÿåº¦ (velocity)
â”‚   â”‚   â””â”€â”€ è½¨è¿¹å†å² (trajectory)
â”‚   â”œâ”€â”€ ç”¨æˆ·çŠ¶æ€ (User State)
â”‚   â”‚   â”œâ”€â”€ ä½ç½® (positions)
â”‚   â”‚   â”œâ”€â”€ ä¿¡å·è´¨é‡ (signal_quality)
â”‚   â”‚   â””â”€â”€ ååé‡å†å² (throughput_history)
â”‚   â””â”€â”€ ä»»åŠ¡çŠ¶æ€ (Mission State)
â”‚       â”œâ”€â”€ å½“å‰æ—¶é—´ (current_time)
â”‚       â”œâ”€â”€ å‰©ä½™æ—¶é—´ (remaining_time)
â”‚       â””â”€â”€ ä»»åŠ¡è¿›åº¦ (progress)
â””â”€â”€ è®­ç»ƒçŠ¶æ€ (Training State)
    â”œâ”€â”€ æ™ºèƒ½ä½“çŠ¶æ€ (Agent State)
    â”œâ”€â”€ è®­ç»ƒè¿›åº¦ (Training Progress)
    â””â”€â”€ æ€§èƒ½æŒ‡æ ‡ (Performance Metrics)
```

### çŠ¶æ€åŒæ­¥æœºåˆ¶

1. **çŠ¶æ€å°è£…**: æ¯å±‚åªæš´éœ²å¿…è¦çš„çŠ¶æ€ä¿¡æ¯
2. **çŠ¶æ€ä¼ é€’**: é€šè¿‡æ˜ç¡®çš„æ¥å£ä¼ é€’çŠ¶æ€
3. **çŠ¶æ€ä¸€è‡´æ€§**: ç¡®ä¿çŠ¶æ€æ›´æ–°çš„åŸå­æ€§å’Œä¸€è‡´æ€§

## ğŸš€ æœ€ä½³å®è·µ

### 1. èŒè´£åˆ†ç¦»
- **Environment**: åªè´Ÿè´£æ¨¡æ‹Ÿï¼Œä¸åšå†³ç­–
- **Agent**: åªè´Ÿè´£å†³ç­–ï¼Œä¸ç®¡ç¯å¢ƒç»†èŠ‚
- **Trainer**: åªè´Ÿè´£è®­ç»ƒç®¡ç†ï¼Œä¸æ¶‰åŠç®—æ³•ç»†èŠ‚

### 2. æ¥å£ç»Ÿä¸€
- æ‰€æœ‰Agentå®ç°ç›¸åŒçš„æ¥å£
- Environmentéµå¾ªOpenAI Gymæ ‡å‡†
- Utilsæä¾›çº¯å‡½æ•°å¼æ¥å£

### 3. é…ç½®ç®¡ç†
- é›†ä¸­ç®¡ç†æ‰€æœ‰é…ç½®å‚æ•°
- æ”¯æŒä¸åŒåœºæ™¯çš„é…ç½®ç»„åˆ
- æä¾›é…ç½®éªŒè¯æœºåˆ¶

### 4. é”™è¯¯å¤„ç†
- æ˜ç¡®çš„é”™è¯¯è¾¹ç•Œå’Œå¤„ç†æœºåˆ¶
- è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œæ—¥å¿—
- ä¼˜é›…çš„é™çº§å’Œæ¢å¤æœºåˆ¶

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### Benchmarkæµ‹è¯•
```python
# Benchmark 1: ç›´çº¿è½¨è¿¹ + ä¼˜åŒ–æ³¢æŸæˆå½¢
config = TrainingConfig(agent_type='benchmark_1')
trainer = Trainer(config)
results = trainer.evaluate()

# Benchmark 2: ç›´çº¿è½¨è¿¹ + éšæœºæ³¢æŸæˆå½¢  
config = TrainingConfig(agent_type='benchmark_2')
trainer = Trainer(config)
results = trainer.evaluate()
```

### RLè®­ç»ƒ
```python
# PPOè®­ç»ƒ
config = TrainingConfig(
    agent_type='ppo',
    learning_rate=3e-4,
    n_steps=2048,
    total_timesteps=100000
)
trainer = Trainer(config)
results = trainer.train()
```

### è‡ªå®šä¹‰ç­–ç•¥
```python
# è‡ªå®šä¹‰åŸºçº¿ç­–ç•¥
config = TrainingConfig(
    agent_type='baseline',
    baseline_strategy='greedy'
)
trainer = Trainer(config)
results = trainer.evaluate()
```

## ğŸ“ˆ ç³»ç»Ÿä¼˜åŠ¿

1. **æ¸…æ™°çš„èŒè´£åˆ†å·¥**: æ¯å±‚ä¸“æ³¨äºç‰¹å®šåŠŸèƒ½
2. **ç»Ÿä¸€çš„æ¥å£è®¾è®¡**: æ˜“äºæ‰©å±•å’Œç»´æŠ¤
3. **æ¨¡å—åŒ–æ¶æ„**: æ”¯æŒç‹¬ç«‹å¼€å‘å’Œæµ‹è¯•
4. **æ ‡å‡†åŒ–è§„èŒƒ**: éµå¾ªRLç¤¾åŒºæœ€ä½³å®è·µ
5. **é«˜å¯é…ç½®æ€§**: æ”¯æŒå¤šç§åœºæ™¯å’Œéœ€æ±‚

è¿™ä¸ªæ¶æ„ç¡®ä¿äº†ç³»ç»Ÿçš„å¯ç»´æŠ¤æ€§ã€å¯æ‰©å±•æ€§å’Œå¯æµ‹è¯•æ€§ï¼Œä¸ºUAV-RLç ”ç©¶æä¾›äº†åšå®çš„æŠ€æœ¯åŸºç¡€ã€‚
