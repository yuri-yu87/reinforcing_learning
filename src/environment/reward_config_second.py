"""
æ¿€è¿›å¢é‡å¥–åŠ±æœºåˆ¶è®¾è®¡
å®Œå…¨ç§»é™¤ç»å¯¹ä½ç½®å¥–åŠ±ï¼Œåªé€šè¿‡è·ç¦»å¢é‡å’Œä»»åŠ¡å®Œæˆé©±åŠ¨è¡Œä¸º
å¼ºè¿«UAVç§»åŠ¨ï¼Œé˜²æ­¢ä»»ä½•å½¢å¼çš„æ‚¬åœåˆ©ç”¨
"""

from dataclasses import dataclass
import numpy as np
from typing import Dict, Any

@dataclass
class RewardConfig:
    """æ¿€è¿›å¢é‡å¥–åŠ±é…ç½® - çº¯å¢é‡é©±åŠ¨ + å¤§å¥–åŠ±å®Œæˆæ¿€åŠ±"""
    
    # æ ¸å¿ƒå¥–åŠ±æƒé‡ (è®­ç»ƒç­–ç•¥ä¼˜åŒ– - å¹³è¡¡å¼•å¯¼ä¸ä»»åŠ¡)
    w_approach: float = 1.0              # è·ç¦»å¢é‡å¥–åŠ±æƒé‡ï¼ˆæ¢å¤æ¸©å’Œå¼•å¯¼ï¼‰
    w_completion: float = 5000.0         # ä»»åŠ¡å®Œæˆå¥–åŠ±æƒé‡ï¼ˆå¤§å¹…æå‡ï¼ï¼‰
    w_time: float = 0.01                 # æ—¶é—´æƒ©ç½šæƒé‡ï¼ˆè¿›ä¸€æ­¥é™ä½ï¼‰
    w_movement_bonus: float = 0.05       # ç§»åŠ¨å¥–åŠ±ï¼ˆæ¢å¤åŸºç¡€æ¿€åŠ±ï¼‰
    w_incomplete_penalty: float = 2.0    # æœªå®Œæˆç”¨æˆ·æƒ©ç½šï¼ˆé€‚ä¸­ï¼‰
    w_position_guidance: float = 0.5     # ä½ç½®å¼•å¯¼å¥–åŠ±ï¼ˆé™ä½ï¼Œé¿å…ä¸è·ç¦»å¥–åŠ±å†²çªï¼‰
    w_constraint_violation: float = 100.0   # çº¦æŸè¿åæƒ©ç½šï¼ˆå¤§å¹…é™ä½ï¼Œå…è®¸å­¦ä¹ ï¼‰
    
    # è®¿é—®åˆ¤å®šæ¡ä»¶ (è·ç¦»+æ—¶é—´åŒé‡æ¡ä»¶)
    visit_distance_threshold: float = 5.0   # è®¿é—®è·ç¦»é˜ˆå€¼
    visit_time_threshold: float = 3.0        # è®¿é—®æ—¶é—´é˜ˆå€¼ï¼ˆä¿ç•™ï¼Œä½†ä¸ä½¿ç”¨ï¼‰
    max_service_time: float = 20.0           # æœ€å¤§æœåŠ¡æ—¶é•¿ï¼ˆé˜²æ­¢æ‚¬åœexploitï¼‰
    
    # ç¯å¢ƒå‚æ•°
    end_position_tolerance: float = 8.0     # ç»ˆç‚¹å®¹å¿èŒƒå›´
    terminal_bonus: float = 3000.0          # ç»ˆç‚¹åˆ°è¾¾å¥–åŠ±ï¼ˆæå¤§æå‡ï¼10å€äºexploitæ”¶ç›Šï¼‰
    
    # çº¦æŸå¼ºåŒ–å­¦ä¹ å‚æ•°
    min_approach_distance: float = 0.5      # æœ€å°æ¥è¿‘è·ç¦»
    approach_reward_cap: float = 2.0        # å¢é‡å¥–åŠ±ä¸Šé™
    max_reward_distance: float = 80.0       # å¥–åŠ±èŒƒå›´ï¼ˆæ¢å¤åˆç†èŒƒå›´ï¼‰
    hover_penalty: float = 2.0              # æ‚¬åœæƒ©ç½šï¼ˆæ¸©å’Œï¼‰
    min_movement_for_reward: float = 0.5    # ç§»åŠ¨å¥–åŠ±æœ€å°è·ç¦»
    position_guidance_range: float = 80.0   # ä½ç½®å¼•å¯¼èŒƒå›´ï¼ˆæ¢å¤ï¼‰
    
    # æ¸©å’Œçº¦æŸå‚æ•°ï¼ˆè®­ç»ƒå‹å¥½ï¼‰
    min_movement_per_window: float = 2.0    # æ¯ä¸ªæ—¶é—´çª—å£æœ€å°ç§»åŠ¨è·ç¦»ï¼ˆå¤§å¹…é™ä½ï¼‰
    movement_check_window: int = 200        # ç§»åŠ¨æ£€æŸ¥çª—å£ï¼ˆæ­¥æ•°ï¼‰ï¼ˆå»¶é•¿ï¼‰
    min_progress_rate: float = 0.02         # æœ€å°è¿›åº¦ç‡ï¼ˆå¤§å¹…é™ä½ï¼‰
    progress_check_interval: int = 300      # è¿›åº¦æ£€æŸ¥é—´éš”ï¼ˆæ­¥æ•°ï¼‰ï¼ˆå¤§å¹…å»¶é•¿ï¼‰
    max_stagnation_steps: int = 500         # æœ€å¤§åœæ»æ­¥æ•°ï¼ˆå¤§å¹…æ”¾å®½ï¼‰
    
    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆç”¨äºç¯å¢ƒä¿¡æ¯æ”¶é›†ï¼‰"""
        return {
            'w_approach': self.w_approach,
            'w_completion': self.w_completion,
            'w_time': self.w_time,
            'w_movement_bonus': self.w_movement_bonus,
            'w_incomplete_penalty': self.w_incomplete_penalty,
            'w_position_guidance': self.w_position_guidance,
            'w_constraint_violation': self.w_constraint_violation,
                    'visit_distance_threshold': self.visit_distance_threshold,
        'visit_time_threshold': self.visit_time_threshold,
        'max_service_time': self.max_service_time,
            'end_position_tolerance': self.end_position_tolerance,
            'terminal_bonus': self.terminal_bonus,
            'min_approach_distance': self.min_approach_distance,
            'approach_reward_cap': self.approach_reward_cap,
            'max_reward_distance': self.max_reward_distance,
            'hover_penalty': self.hover_penalty,
            'min_movement_for_reward': self.min_movement_for_reward,
            'position_guidance_range': self.position_guidance_range
        }


class RewardCalculator:
    """
    æ¿€è¿›å¢é‡å¥–åŠ±è®¡ç®—å™¨
    
    æ ¸å¿ƒç†å¿µï¼š
    1. å®Œå…¨ç§»é™¤ç»å¯¹ä½ç½®å¥–åŠ± - æ–­ç»æ‚¬åœè·åˆ©
    2. ä¸¥æ ¼è·ç¦»å¢é‡å¥–åŠ± - åªæœ‰çœŸæ­£æ¥è¿‘æ‰ç»™å¥–åŠ±
    3. ç§»åŠ¨å¥–åŠ± - é¼“åŠ±ä»»ä½•å½¢å¼çš„ç§»åŠ¨
    4. å¤§å¹…ä»»åŠ¡å®Œæˆå¥–åŠ± - å¼ºåŒ–æ­£ç¡®ç›®æ ‡
    5. å¼ºåŒ–æ—¶é—´å‹åŠ› - é¿å…æ— æ•ˆå¾˜å¾Š
    """
    
    def __init__(self, config: RewardConfig):
        self.config = config
        self.reset(2)
    
    def reset(self, num_users: int):
        """é‡ç½®çŠ¶æ€"""
        self.visited_users = set()
        self.user_entered = set()  # æ–°å¢ï¼šè·Ÿè¸ªå·²è¿›å…¥ç”¨æˆ·åŒºåŸŸçš„çŠ¶æ€
        self.user_close_time = {i: 0.0 for i in range(num_users)}
        self.user_service_time = {i: 0.0 for i in range(num_users)}  # æœåŠ¡æ—¶é•¿è·Ÿè¸ª
        
        # çº¦æŸæ£€æŸ¥æœºåˆ¶
        self.position_history = []  # ä½ç½®å†å²
        self.movement_window = []   # ç§»åŠ¨çª—å£
        self.progress_check_steps = 0  # è¿›åº¦æ£€æŸ¥æ­¥æ•°
        self.last_progress_distance = None  # ä¸Šæ¬¡è¿›åº¦æ£€æŸ¥æ—¶çš„è·ç¦»
        self.stagnation_steps = 0   # åœæ»æ­¥æ•°è®¡æ•°
        self.current_focus_user = 0  # ä»ç”¨æˆ·0å¼€å§‹
        self.all_users_visited = False
        self.user_completion_given = set()
        
        # è·ç¦»è·Ÿè¸ªçŠ¶æ€ï¼ˆç”¨äºå¢é‡å¥–åŠ±ï¼‰
        self.prev_focus_distance = None
        self.prev_goal_distance = None
    

    
    def _check_out_of_bounds(self, uav_position) -> bool:
        """æ£€æŸ¥UAVæ˜¯å¦è¶Šç•Œï¼ˆç®€å•è¾¹ç•Œæ£€æŸ¥ï¼‰"""
        # å‡è®¾é£è¡ŒåŒºåŸŸä¸º [0, 100] x [0, 100] x [0, 100]
        return (uav_position[0] < 0 or uav_position[0] > 100 or
                uav_position[1] < 0 or uav_position[1] > 100 or
                uav_position[2] < 0 or uav_position[2] > 100)
    
    def _get_user_distance(self, uav_position, user_id: int, user_positions) -> float:
        """è®¡ç®—UAVä¸ç”¨æˆ·çš„è·ç¦»"""
        if user_positions is None or user_id >= len(user_positions):
            return float('inf')
        return float(np.linalg.norm(uav_position[:2] - user_positions[user_id][:2]))

    def _check_user_completion(self, uav_position, user_positions, time_step: float):
        """æ£€æŸ¥ç”¨æˆ·è®¿é—®å®Œæˆï¼ˆè¿›å…¥+ç¦»å¼€åŒé‡åˆ¤å®š+æœåŠ¡æ—¶é•¿é™åˆ¶ï¼‰- åªæ£€æŸ¥å½“å‰ä¸“æ³¨ç”¨æˆ·"""
        # æ¸…ç†å·²è®¿é—®ç”¨æˆ·çš„è¿›å…¥çŠ¶æ€å’ŒæœåŠ¡æ—¶é•¿
        for user_id in list(self.user_entered):
            if user_id in self.visited_users:
                self.user_entered.remove(user_id)
                self.user_service_time[user_id] = 0.0  # é‡ç½®æœåŠ¡æ—¶é•¿
        
        # åªæ£€æŸ¥å½“å‰ä¸“æ³¨çš„ç”¨æˆ·ï¼ˆä¸è§‚å¯ŸçŠ¶æ€ä¿æŒä¸€è‡´ï¼‰
        if self.current_focus_user is None or self.current_focus_user >= len(user_positions):
            return None
            
        user_id = self.current_focus_user
        
        # å¦‚æœå½“å‰ä¸“æ³¨ç”¨æˆ·å·²ç»è¢«è®¿é—®ï¼Œè·³è¿‡
        if user_id in self.visited_users:
            return None
            
        distance = self._get_user_distance(uav_position, user_id, user_positions)
        
        # æ£€æŸ¥è¿›å…¥çŠ¶æ€
        if distance <= self.config.visit_distance_threshold:
            if user_id not in self.user_entered:
                self.user_entered.add(user_id)
                self.user_service_time[user_id] = 0.0  # å¼€å§‹è®¡æ—¶
                print(f"ğŸ¯ UAVè¿›å…¥ç”¨æˆ·{user_id}æœåŠ¡åŒºåŸŸï¼Œè·ç¦»={distance:.1f}m")
            else:
                # å·²è¿›å…¥ï¼Œç´¯è®¡æœåŠ¡æ—¶é•¿
                self.user_service_time[user_id] += time_step
                
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§æœåŠ¡æ—¶é•¿ï¼ˆé˜²æ­¢æ‚¬åœexploitï¼‰
                if self.user_service_time[user_id] >= self.config.max_service_time:
                    self.visited_users.add(user_id)
                    self.user_entered.remove(user_id)
                    self.user_service_time[user_id] = 0.0
                    print(f"â° ç”¨æˆ·{user_id}æœåŠ¡æ—¶é•¿è¾¾åˆ°ä¸Šé™ï¼è‡ªåŠ¨å®Œæˆè®¿é—®")
                    return user_id
        
        # æ£€æŸ¥ç¦»å¼€çŠ¶æ€ï¼ˆå·²è¿›å…¥ä¸”ç°åœ¨ç¦»å¼€ï¼‰
        elif user_id in self.user_entered and distance > self.config.visit_distance_threshold:
            self.visited_users.add(user_id)
            self.user_entered.remove(user_id)  # ç«‹å³æ¸…ç†è¿›å…¥çŠ¶æ€
            self.user_service_time[user_id] = 0.0  # é‡ç½®æœåŠ¡æ—¶é•¿
            print(f"âœ… ç”¨æˆ·{user_id}è®¿é—®å®Œæˆï¼(è¿›å…¥â†’ç¦»å¼€)")
            return user_id
        
        return None
    
    def _update_focus(self, uav_position, user_positions):
        """æ›´æ–°ä¸“æ³¨ç”¨æˆ· - ç¨³å®šä¸“æ³¨ç­–ç•¥ï¼Œé¿å…é¢‘ç¹åˆ‡æ¢"""
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç”¨æˆ·éƒ½è®¿é—®å®Œæˆ
        if len(self.visited_users) >= len(user_positions):
            self.all_users_visited = True
            self.current_focus_user = None
            return
        
        # å¦‚æœå½“å‰ä¸“æ³¨ç”¨æˆ·å·²è¢«è®¿é—®ï¼Œæˆ–è€…è¿˜æ²¡æœ‰ä¸“æ³¨ç”¨æˆ·ï¼Œåˆ™é€‰æ‹©æ–°çš„
        if (self.current_focus_user is None or 
            self.current_focus_user in self.visited_users):
            
            # é€‰æ‹©ç¬¬ä¸€ä¸ªæœªè®¿é—®ç”¨æˆ·ï¼ˆç¨³å®šç­–ç•¥ï¼Œé¿å…é¢‘ç¹åˆ‡æ¢ï¼‰
            for user_id in range(len(user_positions)):
                if user_id not in self.visited_users:
                    if user_id != self.current_focus_user:  # åªæœ‰çœŸæ­£åˆ‡æ¢æ—¶æ‰é‡ç½®
                        self.current_focus_user = user_id
                        # é‡ç½®è·ç¦»è·Ÿè¸ªçŠ¶æ€ï¼ˆé˜²æ­¢å¢é‡å¥–åŠ±åŸºäºé”™è¯¯çš„åŸºçº¿ï¼‰
                        self.prev_focus_distance = None
                        # é‡ç½®æ–°ä¸“æ³¨ç”¨æˆ·çš„æœåŠ¡æ—¶é•¿
                        self.user_service_time[user_id] = 0.0
                        print(f"ğŸ¯ ä¸“æ³¨åˆ‡æ¢åˆ°ç”¨æˆ·{user_id}ï¼Œé‡ç½®è·ç¦»è·Ÿè¸ª")
                    break
    
    def calculate_reward(self, uav_position, end_position, user_positions, 
                        user_individual_throughputs=None, prev_position=None, time_step=0.1) -> tuple[float, Dict[str, Any]]:
        """
        æ¿€è¿›å¢é‡å¥–åŠ±è®¡ç®—
        
        Args:
            uav_position: UAVå½“å‰ä½ç½®
            end_position: ç»ˆç‚¹ä½ç½®
            user_positions: ç”¨æˆ·ä½ç½®åˆ—è¡¨
            user_individual_throughputs: ä¸ä½¿ç”¨
            prev_position: UAVä¸Šä¸€æ­¥ä½ç½®ï¼ˆç”¨äºç§»åŠ¨æ£€æµ‹ï¼‰
            time_step: æ—¶é—´æ­¥é•¿
            
        è¿”å›: (æ€»å¥–åŠ±, å¥–åŠ±è¯¦æƒ…)
        """
        
        # 1. æ£€æŸ¥ç”¨æˆ·è®¿é—®å®Œæˆï¼ˆå¤§å¥–åŠ±ï¼‰
        completion_bonus = 0.0
        completed_user = self._check_user_completion(uav_position, user_positions, time_step)
        if completed_user is not None and completed_user not in self.user_completion_given:
            completion_bonus = self.config.w_completion
            self.user_completion_given.add(completed_user)
            print(f"ğŸ‰ ç”¨æˆ·{completed_user}è®¿é—®å®Œæˆï¼å¥–åŠ±={completion_bonus}")
        
        # 2. æ›´æ–°ä¸“æ³¨çŠ¶æ€
        self._update_focus(uav_position, user_positions)
        
        # 3. è®¡ç®—è·ç¦»å¢é‡å¥–åŠ±ï¼ˆå”¯ä¸€æŒç»­å¥–åŠ±ï¼‰
        # æ¢å¤è·ç¦»å¢é‡å¥–åŠ±ï¼ˆæ¸©å’Œå¼•å¯¼ï¼‰
        if self.all_users_visited:
            approach_reward = self._calculate_goal_approach_reward(uav_position, end_position)
            target_info = "Goal"
        else:
            approach_reward = self._calculate_user_approach_reward(uav_position, user_positions)
            target_info = f"User{self.current_focus_user}"
        
        # 4. ç§»åŠ¨å¥–åŠ±ï¼ˆæ¢å¤åŸºç¡€æ¿€åŠ±ï¼‰
        movement_reward = 0.0
        if prev_position is not None:
            displacement = np.linalg.norm(uav_position - prev_position)
            if displacement >= self.config.min_movement_for_reward:
                movement_reward = self.config.w_movement_bonus * displacement
        
        # 5. æœªå®Œæˆç”¨æˆ·æƒ©ç½šï¼ˆæ¸©å’Œé¼“åŠ±è®¿é—®æ‰€æœ‰ç”¨æˆ·ï¼‰
        incomplete_penalty = 0.0
        if not self.all_users_visited and user_positions is not None:
            num_unvisited = len(user_positions) - len(self.visited_users)
            incomplete_penalty = -self.config.w_incomplete_penalty * num_unvisited
        
        # 6. ä½ç½®å¼•å¯¼å¥–åŠ±ï¼ˆæ¸©å’Œå¼•å¯¼åˆ°ç›®æ ‡ï¼‰
        position_guidance = 0.0
        if self.all_users_visited:
            # æœå‘ç»ˆç‚¹çš„å¼•å¯¼
            distance_to_end = np.linalg.norm(uav_position - end_position)
            if distance_to_end <= self.config.position_guidance_range:
                position_guidance = self.config.w_position_guidance * (1.0 - distance_to_end / self.config.position_guidance_range)
        else:
            # æœå‘å½“å‰ä¸“æ³¨ç”¨æˆ·çš„å¼•å¯¼ï¼ˆåªæœ‰æœªè®¿é—®çš„ç”¨æˆ·ï¼‰
            if (self.current_focus_user is not None and 
                self.current_focus_user < len(user_positions) and 
                self.current_focus_user not in self.visited_users):  # å…³é”®ä¿®å¤ï¼šåªå¼•å¯¼æœªè®¿é—®ç”¨æˆ·
                
                focus_user_position = user_positions[self.current_focus_user]
                distance_to_user = np.linalg.norm(uav_position - focus_user_position)
                if distance_to_user <= self.config.position_guidance_range:
                    position_guidance = self.config.w_position_guidance * (1.0 - distance_to_user / self.config.position_guidance_range)
        
        # 7. çº¦æŸè¿åæ£€æŸ¥ï¼ˆç¡¬çº¦æŸé˜²æ­¢exploitï¼‰
        constraint_violation_penalty = self._check_constraints(uav_position, user_positions, time_step)
        
        # 8. æ™ºèƒ½æ‚¬åœæƒ©ç½šï¼ˆæ¸©å’Œé˜»æ­¢æ‚¬åœï¼‰
        hover_penalty = 0.0
        if prev_position is not None:
            displacement = np.linalg.norm(uav_position - prev_position)
            if displacement < 0.5:  # å‡ ä¹æ²¡æœ‰ç§»åŠ¨
                # æ£€æŸ¥æ˜¯å¦åœ¨ä»»ä½•ç”¨æˆ·çš„æœåŠ¡åŒºåŸŸå†…
                in_service_area = False
                if not self.all_users_visited and user_positions is not None:
                    for user_id in range(len(user_positions)):
                        if user_id not in self.visited_users:
                            distance = self._get_user_distance(uav_position, user_id, user_positions)
                            if distance <= self.config.visit_distance_threshold * 1.2:  # ç¨å¤§èŒƒå›´
                                in_service_area = True
                                break
                
                # åªåœ¨éæœåŠ¡åŒºåŸŸæƒ©ç½šæ‚¬åœ
                if not in_service_area:
                    hover_penalty = -self.config.hover_penalty
        
        # 8. æ—¶é—´æƒ©ç½šï¼ˆæ¸©å’Œç´§è¿«æ„Ÿï¼‰
        time_penalty = -self.config.w_time * time_step
        
        # 9. ç»ˆç«¯å¥–åŠ±ï¼ˆå¤§å¹…æå‡ï¼‰
        terminal_bonus = 0.0
        if self.all_users_visited:
            distance_to_end = np.linalg.norm(uav_position - end_position)
            if distance_to_end < self.config.end_position_tolerance:
                terminal_bonus = self.config.terminal_bonus
                print(f"Goal reached! Reward={terminal_bonus}")
        
        # 10. æ€»å¥–åŠ±è®¡ç®—ï¼ˆçº¦æŸå¼ºåŒ–å­¦ä¹ ï¼‰
        total_reward = (approach_reward + movement_reward + completion_bonus + 
                       incomplete_penalty + position_guidance + constraint_violation_penalty + hover_penalty + time_penalty + terminal_bonus)
        
        # 11. å¥–åŠ±è¯¦æƒ…
        breakdown = {
            'approach_reward': float(approach_reward),  # å·²ç§»é™¤
            'movement_reward': float(movement_reward),  # å·²ç§»é™¤
            'completion_bonus': float(completion_bonus),
            'incomplete_penalty': float(incomplete_penalty),
            'position_guidance': float(position_guidance),
            'constraint_violation_penalty': float(constraint_violation_penalty),
            'hover_penalty': float(hover_penalty),
            'time_penalty': float(time_penalty),
            'terminal_bonus': float(terminal_bonus),
            'total_reward': float(total_reward),
            'target': target_info,
            'current_focus_user': self.current_focus_user,
            'visited_users': list(self.visited_users),
            'all_users_visited': self.all_users_visited,
            'user_close_time': dict(self.user_close_time),
            'prev_focus_distance': self.prev_focus_distance,
            'prev_goal_distance': self.prev_goal_distance
        }
        
        return float(total_reward), breakdown
    
    def _check_constraints(self, uav_position, user_positions, time_step):
        """æ£€æŸ¥çº¦æŸè¿å - ç¡¬çº¦æŸé˜²æ­¢exploitè¡Œä¸º"""
        total_penalty = 0.0
        
        # è®°å½•å½“å‰ä½ç½®
        self.position_history.append(uav_position.copy())
        
        # è®¡ç®—å½“å‰ç§»åŠ¨è·ç¦»
        current_movement = 0.0
        if len(self.position_history) > 1:
            current_movement = np.linalg.norm(uav_position - self.position_history[-2])
        
        # ç»´æŠ¤ç§»åŠ¨çª—å£
        self.movement_window.append(current_movement)
        if len(self.movement_window) > self.config.movement_check_window:
            self.movement_window.pop(0)
        
        # çº¦æŸ1: ç§»åŠ¨çº¦æŸ - å¿…é¡»ä¿æŒæœ€ä½ç§»åŠ¨é‡
        if len(self.movement_window) >= self.config.movement_check_window:
            total_movement = sum(self.movement_window)
            if total_movement < self.config.min_movement_per_window:
                penalty = -self.config.w_constraint_violation * (1 + (self.config.min_movement_per_window - total_movement) / self.config.min_movement_per_window)
                total_penalty += penalty
                print(f"âš ï¸ ç§»åŠ¨çº¦æŸè¿åï¼çª—å£ç§»åŠ¨={total_movement:.1f}, è¦æ±‚>{self.config.min_movement_per_window}, æƒ©ç½š={penalty:.1f}")
        
        # çº¦æŸ2: è¿›åº¦çº¦æŸ - å¿…é¡»æœç›®æ ‡å‰è¿›ï¼ˆæ¸©å’Œç‰ˆæœ¬ï¼‰
        self.progress_check_steps += 1
        if self.progress_check_steps >= self.config.progress_check_interval:
            current_target_distance = self._get_current_target_distance(uav_position, user_positions)
            
            if self.last_progress_distance is not None:
                progress = self.last_progress_distance - current_target_distance
                required_progress = self.config.min_progress_rate * self.config.progress_check_interval * time_step
                
                # ä¸¥æ ¼è¿›åº¦è¦æ±‚ï¼šå¿…é¡»æœç›®æ ‡å‰è¿›
                if progress < required_progress:  # ä¸å…è®¸å€’é€€æˆ–åœæ»
                    penalty_factor = min(3.0, abs(progress - required_progress) / required_progress)  # æé«˜æƒ©ç½šå€æ•°
                    penalty = -self.config.w_constraint_violation * penalty_factor
                    total_penalty += penalty
                    print(f"âš ï¸ è¿›åº¦çº¦æŸè¿åï¼å®é™…è¿›åº¦={progress:.1f}, è¦æ±‚>{required_progress:.1f}, æƒ©ç½š={penalty:.1f}")
            
            # é‡ç½®è¿›åº¦æ£€æŸ¥
            self.last_progress_distance = current_target_distance
            self.progress_check_steps = 0
        
        # çº¦æŸ3: åœæ»çº¦æŸ - ä¸èƒ½é•¿æ—¶é—´æ— ç§»åŠ¨
        if current_movement < 0.1:  # å‡ ä¹æ²¡æœ‰ç§»åŠ¨
            self.stagnation_steps += 1
            if self.stagnation_steps > self.config.max_stagnation_steps:
                penalty = -self.config.w_constraint_violation * (1 + (self.stagnation_steps - self.config.max_stagnation_steps) / 100.0)
                total_penalty += penalty
                print(f"âš ï¸ åœæ»çº¦æŸè¿åï¼åœæ»æ­¥æ•°={self.stagnation_steps}, æœ€å¤§={self.config.max_stagnation_steps}, æƒ©ç½š={penalty:.1f}")
        else:
            self.stagnation_steps = 0  # é‡ç½®åœæ»è®¡æ•°
        
        return total_penalty
    
    def _get_current_target_distance(self, uav_position, user_positions):
        """è·å–åˆ°å½“å‰ç›®æ ‡çš„è·ç¦»"""
        if self.all_users_visited:
            # æœå‘ç»ˆç‚¹
            return np.linalg.norm(uav_position - np.array([80, 80, 50]))
        elif self.current_focus_user is not None and self.current_focus_user < len(user_positions):
            # æœå‘å½“å‰ä¸“æ³¨ç”¨æˆ·
            return self._get_user_distance(uav_position, self.current_focus_user, user_positions)
        else:
            # æœå‘æœ€è¿‘çš„æœªè®¿é—®ç”¨æˆ·
            min_distance = float('inf')
            for user_id in range(len(user_positions)):
                if user_id not in self.visited_users:
                    distance = self._get_user_distance(uav_position, user_id, user_positions)
                    min_distance = min(min_distance, distance)
            return min_distance if min_distance != float('inf') else 0.0
    
    def _calculate_user_approach_reward(self, uav_position, user_positions) -> float:
        """è®¡ç®—ç”¨æˆ·é˜¶æ®µçš„ä¸¥æ ¼è·ç¦»å¢é‡å¥–åŠ±"""
        if self.current_focus_user is None or self.current_focus_user >= len(user_positions):
            return 0.0
        
        focus_user_position = user_positions[self.current_focus_user]
        current_distance = np.linalg.norm(uav_position - focus_user_position)
        
        # ä¸¥æ ¼çš„è·ç¦»å¢é‡å¥–åŠ±
        approach_reward = 0.0
        if self.prev_focus_distance is not None:
            distance_improvement = self.prev_focus_distance - current_distance
            
            # ä¸¥æ ¼æ¡ä»¶ï¼šæ˜¾è‘—æ¥è¿‘ + åˆç†è·ç¦»èŒƒå›´ + ä¸èƒ½å¤ªè¿œ
            if (distance_improvement >= self.config.min_approach_distance and 
                current_distance <= self.config.max_reward_distance and
                current_distance > 5.0):  # ä¸èƒ½å¤ªè¿‘ï¼ˆé˜²æ­¢åœ¨ç›®æ ‡å‘¨å›´æŒ¯è¡ï¼‰
                
                approach_reward = min(
                    self.config.w_approach * distance_improvement,
                    self.config.approach_reward_cap
                )
        
        # æ›´æ–°è·ç¦»è®°å½•
        self.prev_focus_distance = current_distance
        
        # è°ƒè¯•è¾“å‡ºï¼ˆæ¯50æ­¥æ˜¾ç¤ºä¸€æ¬¡ï¼‰
        if hasattr(self, '_debug_counter'):
            self._debug_counter += 1
        else:
            self._debug_counter = 0
            
        if self._debug_counter % 50 == 0:
            print(f"ğŸ” ç”¨æˆ·{self.current_focus_user}: è·ç¦»={current_distance:.1f}m, "
                  f"å¢é‡å¥–åŠ±={approach_reward:.3f}")
        
        return approach_reward
    
    def _calculate_goal_approach_reward(self, uav_position, end_position) -> float:
        """è®¡ç®—ç»ˆç‚¹é˜¶æ®µçš„ä¸¥æ ¼è·ç¦»å¢é‡å¥–åŠ±"""
        current_distance = np.linalg.norm(uav_position - end_position)
        
        # ä¸¥æ ¼çš„è·ç¦»å¢é‡å¥–åŠ±
        approach_reward = 0.0
        if self.prev_goal_distance is not None:
            distance_improvement = self.prev_goal_distance - current_distance
            
            # ä¸¥æ ¼æ¡ä»¶ï¼šæ˜¾è‘—æ¥è¿‘ + åˆç†è·ç¦»èŒƒå›´
            if (distance_improvement >= self.config.min_approach_distance and 
                current_distance <= self.config.max_reward_distance and
                current_distance > 5.0):
                
                approach_reward = min(
                    self.config.w_approach * distance_improvement,
                    self.config.approach_reward_cap
                )
        
        # æ›´æ–°è·ç¦»è®°å½•
        self.prev_goal_distance = current_distance
        
        return approach_reward

# æµ‹è¯•æ¿€è¿›å¢é‡å¥–åŠ±æœºåˆ¶
def test_radical_approach_reward():
    config = RewardConfig()
    calculator = RewardCalculator(config)
    
    # æµ‹è¯•åœºæ™¯
    uav_pos = np.array([0.0, 0.0, 50.0])
    end_pos = np.array([80.0, 80.0, 50.0])
    user_positions = np.array([[15.0, 75.0, 0.0], [75.0, 15.0, 0.0]])
    
    print("=== å¹³è¡¡å¢é‡å¥–åŠ±æœºåˆ¶æµ‹è¯• ===")
    
    # æµ‹è¯•1ï¼šé™æ­¢çŠ¶æ€ï¼ˆåº”è¯¥æœ‰æ¸©å’Œæƒ©ç½šå’Œä½ç½®å¼•å¯¼ï¼‰
    print("\n--- æµ‹è¯•1: é™æ­¢çŠ¶æ€ ---")
    reward1, breakdown1 = calculator.calculate_reward(
        uav_pos, end_pos, user_positions
    )
    print(f"é™æ­¢å¥–åŠ±: {reward1:.4f}")
    print(f"å¢é‡å¥–åŠ±: {breakdown1['approach_reward']:.4f}")
    print(f"ç§»åŠ¨å¥–åŠ±: {breakdown1['movement_reward']:.4f}")
    print(f"æœªå®Œæˆæƒ©ç½š: {breakdown1['incomplete_penalty']:.4f}")
    print(f"ä½ç½®å¼•å¯¼: {breakdown1['position_guidance']:.4f}")
    print(f"æ—¶é—´æƒ©ç½š: {breakdown1['time_penalty']:.4f}")
    print(f"æ€»å¥–åŠ±: {breakdown1['total_reward']:.4f}")
    
    # æµ‹è¯•2ï¼šå¤§å¹…æ¥è¿‘ç”¨æˆ·ï¼ˆåº”è¯¥æœ‰æ˜¾è‘—å¥–åŠ±ï¼‰
    print("\n--- æµ‹è¯•2: å¤§å¹…æ¥è¿‘ç”¨æˆ·0 ---")
    new_pos = np.array([10.0, 60.0, 50.0])  # å¤§å¹…å‘ç”¨æˆ·0ç§»åŠ¨
    reward2, breakdown2 = calculator.calculate_reward(
        new_pos, end_pos, user_positions, prev_position=uav_pos
    )
    print(f"å¤§å¹…æ¥è¿‘å¥–åŠ±: {reward2:.4f}")
    print(f"å¢é‡å¥–åŠ±: {breakdown2['approach_reward']:.4f}")
    print(f"ç§»åŠ¨å¥–åŠ±: {breakdown2['movement_reward']:.4f}")
    print(f"æ€»å¥–åŠ±: {breakdown2['total_reward']:.4f}")
    
    # æµ‹è¯•3ï¼šå¾®å°ç§»åŠ¨ï¼ˆåº”è¯¥è¢«æ‹’ç»å¢é‡å¥–åŠ±ï¼‰
    print("\n--- æµ‹è¯•3: å¾®å°ç§»åŠ¨ ---")
    tiny_pos = np.array([10.5, 60.5, 50.0])  # å¾®å°ç§»åŠ¨
    reward3, breakdown3 = calculator.calculate_reward(
        tiny_pos, end_pos, user_positions, prev_position=new_pos
    )
    print(f"å¾®å°ç§»åŠ¨å¥–åŠ±: {reward3:.4f}")
    print(f"å¢é‡å¥–åŠ±: {breakdown3['approach_reward']:.4f}")
    print(f"ç§»åŠ¨å¥–åŠ±: {breakdown3['movement_reward']:.4f}")
    print(f"æ€»å¥–åŠ±: {breakdown3['total_reward']:.4f}")
    
    # æµ‹è¯•4ï¼šç”¨æˆ·è®¿é—®å®Œæˆ
    print("\n--- æµ‹è¯•4: ç”¨æˆ·è®¿é—®å®Œæˆ ---")
    calculator.visited_users.add(0)
    calculator.user_completion_given.add(0)
    reward4, breakdown4 = calculator.calculate_reward(
        np.array([15.0, 75.0, 50.0]), end_pos, user_positions
    )
    print(f"ç”¨æˆ·å®Œæˆå¥–åŠ±: {reward4:.4f}")
    print(f"å®Œæˆå¥–åŠ±: {breakdown4['completion_bonus']:.4f}")
    
    # æµ‹è¯•5ï¼šæ‚¬åœæƒ©ç½š
    print("\n--- æµ‹è¯•5: æ‚¬åœæƒ©ç½š ---")
    hover_pos = np.array([15.0, 75.0, 50.0])  # æ‚¬åœåœ¨åŒä¸€ä½ç½®
    reward5, breakdown5 = calculator.calculate_reward(
        hover_pos, end_pos, user_positions, prev_position=hover_pos
    )
    print(f"æ‚¬åœå¥–åŠ±: {reward5:.4f}")
    print(f"æ‚¬åœæƒ©ç½š: {breakdown5['hover_penalty']:.4f}")
    print(f"æ—¶é—´æƒ©ç½š: {breakdown5['time_penalty']:.4f}")
    print(f"æ€»å¥–åŠ±: {breakdown5['total_reward']:.4f}")
    
    # æµ‹è¯•6ï¼šç»ˆç‚¹åˆ°è¾¾
    print("\n--- æµ‹è¯•6: ç»ˆç‚¹åˆ°è¾¾ ---")
    calculator.all_users_visited = True
    reward6, breakdown6 = calculator.calculate_reward(
        np.array([80.0, 80.0, 50.0]), end_pos, user_positions
    )
    print(f"ç»ˆç‚¹å¥–åŠ±: {reward6:.4f}")
    print(f"ç»ˆç«¯å¥–åŠ±: {breakdown6['terminal_bonus']:.4f}")
    print(f"æ€»å¥–åŠ±: {breakdown6['total_reward']:.4f}")
    
    return calculator


# æµ‹è¯•è·ç¦»å¼•å¯¼å¥–åŠ±æœºåˆ¶
def test_distance_approach_reward():
    config = RewardConfig()
    calculator = RewardCalculator(config)
    
    # æµ‹è¯•åœºæ™¯
    uav_pos = np.array([0.0, 0.0, 50.0])
    end_pos = np.array([80.0, 80.0, 50.0])
    user_positions = np.array([[15.0, 75.0, 0.0], [75.0, 15.0, 0.0]])
    
    print("=== è·ç¦»å¼•å¯¼å¥–åŠ±æœºåˆ¶æµ‹è¯• ===")
    
    # æ¨¡æ‹ŸUAVå‘ç”¨æˆ·0ç§»åŠ¨çš„è¿‡ç¨‹
    print("\n--- æ¨¡æ‹ŸUAVå‘ç”¨æˆ·0æ¥è¿‘è¿‡ç¨‹ ---")
    
    prev_pos = uav_pos
    for i in range(10):
        # æ¨¡æ‹ŸUAVå‘ç”¨æˆ·0ç§»åŠ¨
        target_user = user_positions[0]
        direction = (target_user - uav_pos)
        direction = direction / np.linalg.norm(direction)  # å•ä½æ–¹å‘å‘é‡
        current_pos = prev_pos + direction * 3.0  # æ¯æ­¥ç§»åŠ¨3ç±³
        
        reward, breakdown = calculator.calculate_reward(
            current_pos, end_pos, user_positions, None, prev_pos
        )
        
        distance_to_target = np.linalg.norm(current_pos - target_user)
        print(f"æ­¥éª¤{i+1}: è·ç¦»ç”¨æˆ·0={distance_to_target:.1f}m, å¥–åŠ±={reward:.3f}")
        prev_pos = current_pos
        
        # å¦‚æœå·²ç»å¾ˆæ¥è¿‘ç”¨æˆ·0ï¼Œåœæ­¢æµ‹è¯•
        if distance_to_target < 5.0:
            break
    
    print("\n=== æµ‹è¯•å®Œæˆ ===")
    return True


if __name__ == "__main__":
    test_radical_approach_reward()
